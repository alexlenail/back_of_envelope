{
  
    
        "post0": {
            "title": "Intuition for Second Order Partial Derivatives and the Hessian Matrix",
            "content": "# imports # using Plots; plotlyjs() using PlotlyJS # from IPython.display import HTML # HTML(fig.to_html()) # where fig = plotly.plot(...) . Unable to load WebIO. Please make sure WebIO works for your Jupyter client. For troubleshooting, please see the WebIO/IJulia documentation. . The Hessian matrix appears in the optimization literature, but the intuition for how the Hessian and its inverse transform vectors is opaque to me. Let&#39;s review second order partial derivatives, and then try to build intuition for the Hessian matrix. . For the purpose of this intuition-building exercise, we&#39;ll work with functions $ Reals^2 mapsto Reals^1$. I&#39;ll also use partial derivative notations $ frac{ partial}{ partial y} f(x, y) = frac{ partial f}{ partial y} = f_y$ interchangeably. . 1. Partial Derivatives . Take the $ Reals^2 mapsto Reals^1$ function $f(x, y) = x^2 + 2y^2$. . A partial derivative is the change in an &quot;output&quot; variable (in our case, $f$) with respect to infinitesimal changes in an &quot;input&quot; variable (in our case, $x$ or $y$). For example, $ frac{ partial}{ partial y} f(x, y) = 4y$, which is to say, for any point in the domain, moving infinitsimally in the y direction changes f propotional to 4 times the y coordinate of the starting point point. . f(x, y) = x^2 + 2y^2 x = 6 xlim=[-10, x] ylim=[-10, 10] xs = LinRange(xlim..., 101) ys = LinRange(ylim..., 101) zs = [f(x, y) for x in xs, y in ys] y = 4 dy = 4 f_y(y) = 4y . f_y (generic function with 1 method) . # built interactive plot traces = GenericTrace[] push!(traces, PlotlyJS.surface(x=xs, y=ys, z=zs, showscale=false, opacity=0.8)) push!(traces, PlotlyJS.surface(x=[x, x+0.001], y=ylim, z=[[maximum(zs), minimum(zs)] [maximum(zs), minimum(zs)]], showscale=false, colorscale=&quot;Greys&quot;, opacity=0.2)) push!(traces, PlotlyJS.scatter3d(x=fill(x, size(ys)), y=ys, z=[x^2 + 2y^2 for y in ys], showlegend=false, mode=&quot;lines&quot;, line=attr(color=&quot;red&quot;, width=2))) for y in ys[1:5:end] push!(traces, PlotlyJS.scatter3d(x=fill(x, 2),y=[y-dy, y+dy], z=[f(x,y)-f_y(y)*dy, f(x,y)+f_y(y)*dy], visible=false, showlegend=false, mode=&quot;lines&quot;, line=attr(color=&quot;orange&quot;, width=5))) end scene = attr( xaxis = attr(range=[-10,10]), yaxis = attr(range=[-10,10]), zaxis = attr(range=[-50,300]), aspectratio = attr(x=1, y=1, z=1) ) layout = Layout( sliders=[attr( steps=[ attr( label=round(y, digits=2), method=&quot;update&quot;, args=[attr(visible=[fill(true, 3); fill(false, i-1); true; fill(false, 101-i)])] ) for (i, y) in enumerate(ys[1:5:end]) ], active = y, currentvalue_prefix=&quot;x = 6, y = &quot;, # pad_t=40 )], scene = scene, ) p = PlotlyJS.plot(traces, layout) . We can plot the function $f_y$ for every starting point: . # plot partial derivative of f with respect to y, f_y traces = GenericTrace[] push!(traces, PlotlyJS.surface(x=xs, y=ys, z=zs, showscale=false, opacity=0.8)) push!(traces, PlotlyJS.surface(x=ylim, y=ylim, z=[[0, 0] [0, 0]], showscale=false, colorscale=&quot;Greys&quot;, opacity=0.3)) push!(traces, PlotlyJS.surface(x=xs, y=ys, z=[f_y(y) for x in xs, y in ys], showscale=false)) plot(traces, Layout(scene=scene)) . We can do the exact same exercise with $f_x$: . f_x(x) = 2x . f_x (generic function with 1 method) . traces = GenericTrace[] push!(traces, PlotlyJS.surface(x=xs, y=ys, z=zs, showscale=false, opacity=0.8)) push!(traces, PlotlyJS.surface(x=ylim, y=ylim, z=[[0, 0] [0, 0]], showscale=false, colorscale=&quot;Greys&quot;, opacity=0.3)) push!(traces, PlotlyJS.surface(x=xs, y=ys, z=[f_x(x) for x in xs, y in ys], showscale=false)) plot(traces, Layout(scene=scene)) . So the way the second order partial derivative is defined is as a composition, e.g. $f_{xx} = frac{ partial}{ partial x} left( frac{ partial}{ partial x} left( f(x, y) right) right) $. As second derivatives do, it captures the [change in the [change in the [output variable]]] with respect to infinitesimal changes in the input variable. This notion coincides with the curvature of the function: a positive second derivative at a particular point indicates that the output variable is concave up at a that point, and a negative second derivative indicates the output variable is concave down at that point. . In the case of the function we&#39;ve chosen, $f_{xx} = 2$ and $f_{yy} = 4$ which informs us that $f$ is concave up everywhere in the domain, which makes sense from looking at the plot. . However, we&#39;ve omitted the &quot;mixed&quot; partial derivatives here: $f_{xy}$ and $f_{yx}$. We can compute them to both be zero for this particular function. What does that tell us? . # plot partial derivative of f with respect to y, f_y traces = GenericTrace[] push!(traces, PlotlyJS.surface(x=ylim, y=ylim, z=[[0, 0] [0, 0]], showscale=false, colorscale=&quot;Greys&quot;, opacity=0.3)) push!(traces, PlotlyJS.surface(x=xs, y=ys, z=[f_y(y) for x in xs, y in ys], showscale=false)) push!(traces, PlotlyJS.scatter3d(x=[-7, 4], y=[5, 5], z=[f_y(5), f_y(5)], showlegend=false, mode=&quot;lines&quot;, line=attr(color=&quot;orange&quot;, width=5))) p = plot(traces, Layout(scene=scene)) . It&#39;s clear from the graph above that infinitesimal changes in $x$ do not influence the value of $f_y$. Interpreting $f_{yx} = frac{ partial}{ partial x} left( frac{ partial}{ partial y} left( f(x, y) right) right)$ as $ frac{ partial}{ partial x} f_y $, it&#39;s clear $f_{yx} = 0$. But how can we interpret that in terms of the original function f? . # built interactive plot traces = GenericTrace[] push!(traces, PlotlyJS.surface(x=xs, y=ys, z=zs, showscale=false, opacity=0.8)) for x in xs[1:5:end] push!(traces, PlotlyJS.surface(x=[x, x+0.001], y=ylim, z=[[maximum(zs), minimum(zs)] [maximum(zs), minimum(zs)]], visible=false, showscale=false, colorscale=&quot;Greys&quot;, opacity=0.2)) push!(traces, PlotlyJS.scatter3d(x=fill(x, size(ys)), y=ys, z=[x^2 + 2y^2 for y in ys], visible=false, showlegend=false, mode=&quot;lines&quot;, line=attr(color=&quot;red&quot;, width=2))) push!(traces, PlotlyJS.scatter3d(x=fill(x, 2),y=[y-dy, y+dy], z=[f(x,y)-f_y(y)*dy, f(x,y)+f_y(y)*dy], visible=false, showlegend=false, mode=&quot;lines&quot;, line=attr(color=&quot;orange&quot;, width=5))) end layout = Layout( sliders=[attr( steps=[ attr( label=round(x, digits=2), method=&quot;update&quot;, args=[attr(visible=[fill(true, 1); fill(false, 3*(i-1)); fill(true, 3); fill(false, 3*(101-i))])] ) for (i, x) in enumerate(xs[1:5:end]) ], active = x, currentvalue_prefix=&quot;x = 6, y = &quot;, # pad_t=40 )], scene = scene, ) p = PlotlyJS.plot(traces, layout) . Here is where we find the intuition. The mixed second order partial derivatives tell us how the slope along one coordinate axis changes as we move infinitesimally along an orthogonal coordinate axis. In the function we&#39;ve chosen, the slice of the graph at any x coordinate is the same parabola (just vertically offset by $x^2$) and thus has the same slope for any y. The mixed second order partial derivatives could thus be said to relay information about the &quot;pucker&quot; of the graph of the function (I made that name up) which is the concavity of the graph with respect to two coordinate axes. . In order to see this more clearly, let&#39;s add a mixed term to our function, to produce non-zero mixed second order partial derivatives: . g(x, y) = x^2 + 2y^2 - x*y zs = [g(x, y) for x in xs, y in ys] g_x(x, y) = 2x - y g_y(x, y) = 4y - x . g_y (generic function with 1 method) . # built interactive plot traces = GenericTrace[] push!(traces, PlotlyJS.surface(x=xs, y=ys, z=zs, showscale=false, opacity=0.8)) for x in xs push!(traces, PlotlyJS.surface(x=[x, x+0.001], y=ylim, z=[[maximum(zs), minimum(zs)] [maximum(zs), minimum(zs)]], visible=false, showscale=false, colorscale=&quot;Greys&quot;, opacity=0.2)) push!(traces, PlotlyJS.scatter3d(x=fill(x, size(ys)), y=ys, z=[g(x, y) for y in ys], visible=false, showlegend=false, mode=&quot;lines&quot;, line=attr(color=&quot;red&quot;, width=2))) push!(traces, PlotlyJS.scatter3d(x=fill(x, 2),y=[y-dy, y+dy], z=[g(x,y)-g_y(x, y)*dy, g(x,y)+g_y(x, y)*dy], visible=false, showlegend=false, mode=&quot;lines&quot;, line=attr(color=&quot;orange&quot;, width=5))) end layout = Layout( sliders=[attr( steps=[ attr( label=round(x, digits=2), method=&quot;update&quot;, args=[attr(visible=[fill(true, 1); fill(false, 3*(i-1)); fill(true, 3); fill(false, 3*(101-i))])] ) for (i, x) in enumerate(xs) ], active = x, currentvalue_prefix=&quot;x = 6, y = &quot;, # pad_t=40 )], scene = scene, ) p = PlotlyJS.plot(traces, layout) . The slope along the y coordinate changes as we vary x. This function is &quot;puckered&quot; up. . Note: For twice-continuously differentiable functions, $f_{xy}$ = $f_{yx}$. This is called the symmetry property of second derivatives. And leads to a symmetric Hessian matrix. We&#39;ll assume that property of our functions throughout. | . 2. Hessian Matrix . Having now built intuition for each of the entries in the Hessian Matrix, we can begin to build intuition for the matrix itself: . $$ bold{H}_f = begin{bmatrix} frac{ partial^2 f}{ partial x^2} &amp; frac{ partial^2 f}{ partial x partial y} frac{ partial^2 f}{ partial y partial x} &amp; frac{ partial^2 f}{ partial y^2} end{bmatrix} = begin{bmatrix} f_{xx} &amp; f_{yx} f_{xy} &amp; f_{yy} end{bmatrix}$$In some sense, the Hessian is a matrix-valued function we can evaluate for any point $ bold{v_0} = begin{bmatrix} x_0 y_0 end{bmatrix}$ in the domain of $f$: . $$ bold{H}_f( bold{v_0}) = begin{bmatrix} f_{xx}( bold{v_0}) &amp; f_{yx}( bold{v_0}) f_{xy}( bold{v_0}) &amp; f_{yy}( bold{v_0}) end{bmatrix}$$The Hessian shows up in the quadratic approximation (second order Taylor expansion) of multivariate functions around a particular point $ bold{v_0}$: . $$ Q_f( bold{v_0}) = color{green} f( bold{v_0}) color{black} + color{blue} nabla_f( bold{v_0}) cdot ( bold{v} - bold{v_0}) color{black} + color{indigo} {1 over 2}( bold{v} - bold{v_0})^ intercal left[ bold{H}_f( bold{v_0}) right] ( bold{v} - bold{v_0})$$ . And we might ask: how does this matrix scale $( bold{v} - bold{v_0})$? Let&#39;s try to build some intuition: . Suppose we multiply some vector against that Hessian Matrix: . $$ begin{bmatrix} f_{xx}( bold{v_0}) &amp; f_{yx}( bold{v_0}) f_{xy}( bold{v_0}) &amp; f_{yy}( bold{v_0}) end{bmatrix} cdot begin{bmatrix} x y end{bmatrix} = begin{bmatrix} f_{xx} cdot x + f_{xy} cdot y f_{yy} cdot y + f_{yx} cdot x end{bmatrix} = $$$$ begin{bmatrix} ( text{the rate of change of the slope in the x direction as you move in the x direction} ) ( text{a distance in the x direction} ) + ( text{the rate of change of the slope in the x direction as you move in the y direction} ) ( text{a distance in the y direction} ) ( text{the rate of change of the slope in the y direction as you move in the y direction} ) ( text{a distance in the y direction} ) + ( text{the rate of change of the slope in the y direction as you move in the x direction} ) ( text{a distance in the x direction} ) end{bmatrix} = $$$$ begin{bmatrix} text{the approximate slope in the x direction at the distance x from }x_0 text{the approximate slope in the y direction at the distance y from }y_0 end{bmatrix} = $$ $$ text{what a second-order approximation of } f text{ suggests the gradient is at } bold{v_0} + begin{bmatrix} x y end{bmatrix} $$ . However, let&#39;s note that the quadratic approximation of $f$ includes as quadratic form $( bold{v} - bold{v_0})^ intercal left[ bold{H}_f( bold{v_0}) right] ( bold{v} - bold{v_0})$, not just a matrix-vector product. Applying this second product operation results in a scalar, which is the change in the value of the function as a result of extrapolating the (second-order) curvature of the function at $ bold{v_0}$ out towards $ bold{v} - bold{v_0}$. . 3. Newton Method . Newton&#39;s method for optimization is an iteration of . forming the quadratic approximation $Q_f( bold{v_0})$ of a function around the current point. | jumping to the min/max of the paraboloid approximation for the next iteration. | . This requires us to identify the min/max of the second-order approximation of $f$. Recall, the quadratic approximation (second order taylor expansion) of $f$ at a particular point $ bold{v_0}$ is . $$ Q_f( bold{v_0}) = color{green} f( bold{v_0}) color{black} + color{blue} nabla_f( bold{v_0}) cdot ( bold{v} - bold{v_0}) color{black} + color{indigo} {1 over 2}( bold{v} - bold{v_0})^ intercal left[ bold{H}_f( bold{v_0}) right] ( bold{v} - bold{v_0})$$ . Searching for the extrema of this approximation, we can take $ nabla left[ Q_f(v_0) right] = 0$. . $ nabla left[ color{green} f( bold{v_0}) color{black} right] = 0$ | $ nabla left[ color{blue} nabla_f( bold{v_0}) cdot ( bold{v} - bold{v_0}) color{black} right] = nabla_f( bold{v_0})$ | $ nabla left[ color{indigo} {1 over 2}( bold{v} - bold{v_0})^ intercal left[ bold{H}_f( bold{v_0}) right] ( bold{v}- bold{v_0}) color{black} right] = {1 over 2} left( left[ bold{H}_f( bold{v_0}) right] bold{v}+ left[ bold{H}_f( bold{v_0}) right]^ intercal right) bold{v}$ | . We can simplify ${1 over 2} left( left[ bold{H}_f( bold{v_0}) right] bold{v}+ left[ bold{H}_f( bold{v_0}) right]^ intercal right) bold{v}$ further, because $ left[ bold{H}_f( bold{v_0}) color{black} right] = left[ bold{H}_f( bold{v_0}) right]^ intercal$, because of the symmetry property of second derivatives, so this expression simplifies to ${1 over 2} left( 2 left[ bold{H}_f( bold{v_0}) right] right) bold{v} = left[ bold{H}_f( bold{v_0}) right] bold{v}$. . So all together, $ nabla left[ Q_f( bold{v_0}) right] = color{blue} nabla_f( bold{v_0}) color{black} + color{indigo} left[ bold{H}_f( bold{v_0}) right] bold{v}$. . Well, $ nabla_f( bold{v_0}) + left[ bold{H}_f( bold{v_0}) right] bold{v} = 0$ when: . $$ begin{aligned} left[ bold{H}_f( bold{v_0}) right] bold{v} &amp;= - nabla_f( bold{v_0}) bold{v} &amp;= - left[ bold{H}_f( bold{v_0}) right]^{-1} cdot nabla_f( bold{v_0}) end{aligned} $$So $ bold{v}$ is the vector which denotes the extremum of the quadratic approximation of $f$ around $ bold{v_0}$. The familiar Newton method expression pops out of our above derivation: . $$ bold{v_{k+1}} = bold{v_k} - left[ bold{H}_f( bold{v_k}) right]^{-1} nabla_f( bold{v_k})$$ . And now we know that this expression which seemed to come out of nowhere $ left[ bold{H}_f( bold{v_k}) right]^{-1} nabla_f( bold{v_k})$ including an inverse Hessian (!!) is really just the $ bold{v}$ which solves $ nabla left[ Q_f(v_0) right] = 0$. . References . Khan Academy: Second-Order Partial Derivatives | Walter Schreiner&#39;s course notes: intuition for second order partial derivatives | Khan Academy: The Hessian | Khan Academy: Quadratic Approximations | Kris Hauser&#39;s course notes: derivation of Newton&#39;s Method (especially page 4) | .",
            "url": "https://alexlenail.me/back_of_my_envelope/2021/05/24/Hessian-Intuition.html",
            "relUrl": "/2021/05/24/Hessian-Intuition.html",
            "date": " • May 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Enzyme Kinetic Parameter Inference",
            "content": "# imports from itertools import combinations_with_replacement, product from collections import Counter, namedtuple from io import StringIO import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.ticker as mtick import matplotlib.patches as patches from matplotlib.colors import to_hex import scipy.stats import seaborn as sns from scipy.stats import multivariate_normal from scipy.interpolate import interp1d from scipy.stats.kde import gaussian_kde from sklearn.linear_model import LinearRegression from sklearn.gaussian_process.kernels import Matern import ipywidgets as widgets from IPython.display import display %config InlineBackend.figure_format = &#39;retina&#39; # from IPython.display import set_matplotlib_formats %matplotlib inline # set_matplotlib_formats(&#39;svg&#39;) plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] plt.rcParams[&#39;figure.dpi&#39;] = 140 plt.rcParams[&#39;agg.path.chunksize&#39;] = 10000 exp = np.exp sqrt = np.sqrt Π = np.prod π = np.pi N = np.random.normal def hex_to_rgb(h): return [int(h.lstrip(&#39;#&#39;)[i:i+2], 16)/256 for i in (0, 2, 4)] . # resize figure special function from IPython.core.display import Image, HTML import io import binascii def resize_fig(width, height): s = io.BytesIO() plt.savefig(s, format=&#39;png&#39;, bbox_inches=&quot;tight&quot;, dpi=200) plt.close() return HTML(f&#39;&lt;img width=&quot;{width}&quot; height=&quot;{height}&quot; class=&quot;keep_dims&quot; src=&quot;data:image/png;base64,{binascii.b2a_base64(s.getvalue()).decode()}&amp;#10;&quot;&gt;&#39;) . 1. Background . $$ newcommand{ kon}{k_{ mathrm{on}}} newcommand{ koff}{k_{ mathrm{off}}} newcommand{ kcat}{k_{ mathrm{cat}}} newcommand{ kuncat}{k_{ mathrm{uncat}}} newcommand{ kms}{k_{m, mathrm{S}}} newcommand{ kmp}{k_{m, mathrm{P}}} newcommand{ dSdt}{ frac{d[ mathrm{S}]}{dt}} newcommand{ dEdt}{ frac{d[ mathrm{E}]}{dt}} newcommand{ dESdt}{ frac{d[ mathrm{ES}]}{dt}} newcommand{ dPdt}{ frac{d[ mathrm{P}]}{dt}}$$1.1 Enzyme Kinetics . Enzymes catalyze many critical chemical reactions in cells. . Describing a cell with a mathematical model (a long-standing goal of computational biologists) would entail modelling each enzyme-catalyzed chemical reaction. . However, although we may know the scheme for many enzymatic reactions (the responsible enzyme, the associated substrates, and resultant products) we are often missing many of the details needed to construct a faithful mathematical model of the reaction. . Let&#39;s begin by introducing the mathematical model used to describe enzymatic reaction schemes. Consider the following enzymatically-catalyzed (uni uni) chemical reaction scheme: . $$ E+S underset{ koff}{ overset{ kon}{ rightleftarrows}} ES underset{ kuncat}{ overset{ kcat}{ rightleftarrows}}E+P $$ . In this scheme E is an enzyme, S is its substrate, ES is the enzyme-substrate complex, which is an intermediate, and P is the product of the reaction. Each of those chemical species has a concentration in a fixed volume, which we denote with brackets (e.g. $[ mathrm{E}]$ = enzyme concentration). . If we make the simplifying assumption that the 4 molecular species are &#39;well-mixed&#39; in solution, we can invoke the &#39;Law of Mass Action&#39; under which the rate of each of the four included reactions is linear in the concentrations of the reactants (with an associated coefficient called the rate constant). The reactions in the above scheme are: enzyme-substrate association ($ kon$), dissociation ($ koff$), enzyme catalysis of substrate into product ($ kcat$), and enzyme-product re-association (&quot;uncatalysis&quot;, $ kuncat$). The designation of &#39;substrate&#39; and &#39;product&#39; is our choice -- the model is entirely symmetric, which is reflected in the associated ODEs: . $$ begin{aligned} frac{d[ mathrm{S}]}{dt} &amp;= k_{ mathrm{off}}[ mathrm{ES}] - k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] frac{d[ mathrm{E}]}{dt} &amp;= k_{ mathrm{off}}[ mathrm{ES}] - k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] + k_{ mathrm{cat}}[ mathrm{ES}] - k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] frac{d[ mathrm{ES}]}{dt} &amp;= - k_{ mathrm{off}}[ mathrm{ES}] + k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] - k_{ mathrm{cat}}[ mathrm{ES}] + k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] frac{d[ mathrm{P}]}{dt} &amp;= k_{ mathrm{cat}}[ mathrm{ES}] - k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] end{aligned}$$This differential equation model describing the (deterministic) chemical kinetics for an enzymatically-catalyzed reaction in well-mixed conditions contains 4 kinetic parameters, i.e. 4 degrees of freedom, which we do not know a priori. These will be the subject of inference. . Note: the intracellular environment is not best described as well-mixed, and models of &#8217;Macromolecular Crowding&#8217; have led to more accurate rate laws for these reactions in vivo. However, we will retain the well-mixed assumption for now. . 1.2 Parameter Inference . There are 3 typical problems associated with ODE models: . Supplied with a complete specification of the system, the forward problem is to integrate the differential equations from some initial conditions forwards in time and predict the trajectory of the system. This is what is typically meant by &quot;solving&quot; the ODE system, but exact analytical solutions are rare, and numerical methods are often brought to bear to approximate system trajectories. | Supplied with one or more trajectories (data) but incomplete specification of the system, the inverse problem is to estimate parameters of the system (coefficients in the ODE expressions). | Finally, given some manipulable inputs, the control problem is to drive the system towards some desired state. | . This post will explore a range of approaches for the inverse problem. Our goal will be to estimate the kinetic parameters of enzymatically-catalyzed chemical reactions from timeseries of concentrations of the molecular species. . Note: enzyme kinetic parameters are typically not inferred from metabolite timeseries data using the methods we will describe, but instead from specific enzyme assays. However, at the moment, these assays are limited to studying one enzyme at a time. The inference approaches described in this post can leverage data from emerging high-throughput assays. . The determination of the kinetic parameters for the enzymatic reactions of life is a major project, and reported values have been tabulated in databases such as BRENDA. However, my experience with these databases has been that the reported kinetic parameters are not internally consistent. . 1.3 The Michaelis-Menten/Briggs-Haldane Approximation . Two assumptions commonly made at this point are: . to assume the initial substrate concentration is much larger than the enzyme concentration ($[ mathrm{S_0}] gg [ mathrm{E_0}]$). | to suppose that the rates of enzyme-substrate association ($ kon$) and dissociation ($ koff$) are greater than the rates of catalysis and uncatalysis (i.e. $ kon$, $ koff$ $ gg$ $ kcat$, $ kuncat$). | These assumptions permit a timescale separation argument called the &quot;Quasi-Steady-State Approximation&quot; (QSSA) in which we set $ dESdt = 0$, which enables the derivation of the traditional Reversible Michaelis-Menten/Briggs-Haldane expression: . $$ begin{aligned} frac{d[ mathrm{P}]}{dt} &amp;= frac{ frac{ kcat , [ mathrm{E_T}] [ mathrm{S}]}{K_{m, mathrm{S}}} - frac{ koff , [ mathrm{E_T}] [ mathrm{P}]}{K_{m, mathrm{P}}}} {1+ frac{[ mathrm{S}]}{K_{m, mathrm{S}}} + frac{[ mathrm{P}]}{K_{m, mathrm{P}}}} frac{d[ mathrm{S}]}{dt} &amp;= - frac{d[ mathrm{P}]}{dt} end{aligned}$$in which we have introduced the &quot;Michaelis Constants&quot;: $K_{m, mathrm{S}} = frac{ koff + kcat}{ kon}$ and $K_{m, mathrm{P}} = frac{ koff + kcat}{ kuncat}$. . The QSSA reduces the system from 4 variables to 2. However, there are still 4 kinetic parameters to estimate in this reduced model. . Note: another assumption typically made at this point is to assume that catalysis is irreversible ($ kuncat = 0$), leading to a further simplified expression for the rate of product formation $ frac{d[ mathrm{P}]}{dt}$. However, this assumption is quite often inaccurate, so we will not make it. . 2. Exploring the Forward Model . 2.1 A Standard Example . Before we explore techniques to estimate enzyme kinetic parameters from timeseries data, we need to generate timeseries data to begin with. We can accomplish that by fixing kinetic parameters, then solving the forward problem. It will turn out that integrating the differential equations forwards is a subroutine of both approaches to the inverse problem we&#39;ll see in this post, so developing a method for the forward problem is hardly wasted effort. . In order to produce a trajectory, we need to set initial conditions. We&#39;ll integrate the reaction kinetics of a hypothetical in vitro experiment, in which a fixed quantity of enzyme and substrate are added to the reaction at the outset, then left to react. . Note: in vivo we would expect the concetration of enzyme to vary over time, and the substrate to be replenished. We will generalize this approach to a more biologically-relevant setting in a future post. . Our initial conditions are: . $[E]_0$, the initial enzyme concentration, is set to 1mM (miliMolar, i.e. 1000μM). | $[S]_0$, the initial substrate concentration is set to 10mM. | . default_initial_conditions = { &#39;S_0&#39;: 10e3, &#39;E_0&#39;: 1e3, &#39;ES_0&#39;: 0.0, &#39;P_0&#39;: 0.0 } . Next, let&#39;s fix some generic rate constants: . $ kon ,$ of $10^6$ events per Mol per second, or 1 per μM per second, is a typical rate for enzyme-substrate binding. | $ koff ,$ of 500/s results in a $ koff$/$ kon$ = $k_d$ of 500 μM, which is a typical $k_d$. | $ kcat ,$ is 30/s, a fairly slow but respectable $ kcat$. | $ kuncat ,$ of $ frac{ kon}{10}$ is often considered as the boundary for the QSSA to hold (so 0.1 per μM per second). Let&#39;s use $ kuncat = frac{ kon}{100} = $ 0.01/μM for good measure. | . Our units are μM and seconds. . default_kinetic_params = { &#39;k_on&#39;: 1, &#39;k_off&#39;: 500, &#39;k_cat&#39;: 30, &#39;k_uncat&#39;: 0.01 } def k_ms(p): return (p[&#39;k_off&#39;] + p[&#39;k_cat&#39;]) / p[&#39;k_on&#39;] def k_mp(p): return (p[&#39;k_off&#39;] + p[&#39;k_cat&#39;]) / p[&#39;k_uncat&#39;] default_kinetic_params[&#39;k_ms&#39;] = k_ms(default_kinetic_params) default_kinetic_params[&#39;k_mp&#39;] = k_mp(default_kinetic_params) . To simulate the kinetics with little derivative steps, we need a step size, and a number of total steps: . dt = 1e-6 steps = 5e5 . There are a variety of numerical methods to integrate systems of differential equations. The most straightforward is Euler&#39;s method, which we&#39;ve written down explicitly for this system below: . # define euler_full(), which integrates the full kinetics with Euler&#39;s Method, and returns a trajectory def integrate_euler_full(kinetic_params, dt=dt, steps=steps, initial_conditions=default_initial_conditions): S, E, ES, P = initial_conditions.values() k_on, k_off, k_cat, k_uncat, k_ms, k_mp = kinetic_params.values() traj = [[S, E, ES, P]] for _ in range(int(steps)): dS = k_off * ES - k_on * E * S dE = k_off * ES - k_on * E * S + k_cat * ES - k_uncat * E * P dES = k_on * E * S - k_off * ES - k_cat * ES + k_uncat * E * P dP = k_cat * ES - k_uncat * E * P S += dS * dt E += dE * dt ES += dES * dt P += dP * dt traj.append([S, E, ES, P]) return pd.DataFrame(traj, columns=[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;], index=np.around(np.linspace(0, dt*steps, int(steps)+1), 6)) . We&#39;ll also write down Euler&#39;s method for the Michaelis-Menten/Briggs-Haldane kinetics . # define euler_MM(), which integrates the Michaelis-Menten/Briggs-Haldane kinetics def integrate_euler_MM(kinetic_params, dt=dt, steps=steps, initial_conditions=default_initial_conditions): S, E, ES, P = initial_conditions.values() k_on, k_off, k_cat, k_uncat, k_ms, k_mp = kinetic_params.values() traj = [P] for _ in range(int(steps)): dP = ((k_cat * E * S) / k_ms - (k_off * E * P) / k_mp) / (1 + S / k_ms + P / k_mp) dS = -dP P += dP * dt S += dS * dt traj.append(P) return pd.Series(traj, name=&#39;P_MM&#39;, index=np.around(np.linspace(0, dt*steps, int(steps)+1), 6)).to_frame() . Now we can integrate the reaction kinetics, and plot the trajectory. We&#39;ll overlay the Michaelis-Menten/Briggs-Haldane kinetics with dotted lines on top of the full kinetics (solid). . traj_euler_full = integrate_euler_full(default_kinetic_params) traj_euler_mm = integrate_euler_MM(default_kinetic_params) . # figure styles def fig_style(ax): for side in [&quot;right&quot;,&quot;top&quot;]: ax.spines[side].set_visible(False) ax.set_xlabel(&#39;time (s)&#39;, weight=&#39;bold&#39;) ax.set_ylabel(&#39;concentration (μM)&#39;, weight=&#39;bold&#39;) def param_string(E_0=None, S_0=None, k_on=None, k_off=None, k_cat=None, k_uncat=None, k_ms=None, k_mp=None, **kwargs): return f&#39;[k_on= {k_on}/μM/s] [k_off = {k_off}/s] [k_cat = {k_cat}/s] [k_uncat = {k_uncat}/μM/s] [E₀ = {int(E_0)}μM] [S₀ = {int(S_0)}μM]&#39; c = { &#39;S&#39;: &#39;dodgerblue&#39;, &#39;E&#39;: &#39;sienna&#39;, &#39;ES&#39;: &#39;blue&#39;, &#39;P&#39;: &#39;darkblue&#39;, &#39;S_MM&#39;: &#39;steelblue&#39;, &#39;P_MM&#39;: &#39;slateblue&#39;, &#39;k_on&#39;: &#39;mediumseagreen&#39;, &#39;k_off&#39;: &#39;olive&#39;, &#39;k_cat&#39;: &#39;darkgreen&#39;, &#39;k_uncat&#39;: &#39;darkgoldenrod&#39;, &#39;k_m&#39;: &#39;olivedrab&#39;, &#39;k_ms&#39;: &#39;forestgreen&#39;, &#39;k_mp&#39;: &#39;darkkhaki&#39;, } c = {k:to_hex(v) for k,v in c.items()} def color(columns): return [c[col] for col in columns] . ax = traj_euler_full.plot.line(title=param_string(**default_initial_conditions, **default_kinetic_params), color=color(traj_euler_full.columns)) traj_euler_mm.plot.line(ax=ax, color=color(traj_euler_mm.columns), linestyle=&#39;--&#39;) fig_style(ax) . We can plainly see the validity of the Quasi-Steady-State Approximation (QSSA) in action in the trajectory: Enzyme E and Substrate S rapidly form Enzyme-Substrate complex ES, the concentration of which remains relatively constant throughout the course of the reaction (recall the QSSA is the approximation that $ dESdt = 0$). Thus, the Michaelis-Menten/Briggs-Haldane product concentration trajectory P_MM well approximates the full kinetics trajectory for the concentration of product P, since the requisite assumptions are valid, namely, (1) $[ mathrm{S_0}] gg [ mathrm{E_0}]$ and (2) $ kon$, $ koff$ $ gg$ $ kcat$, $ kuncat$. . In practice, Michaelis-Menten/Briggs-Haldane kinetics are often assumed by default, risking the possibility of their misapplication. Let&#39;s take this opportunity to explore how the MM/BH kinetics diverge from the full kinetics when we violate the requisite assumptions. . 2.2: Breaking the Michaelis-Menten/Briggs-Haldane Assumptions: &nbsp; &nbsp; &nbsp; Initial Substrate:Enzyme Ratio . Suppose first the number of molecules of substrate is not much greater than the number of molecules of enzyme, which is a plausible regime for certain reactions in vivo. . initial_conditions = { &#39;S_0&#39;: 2e3, &#39;E_0&#39;: 1e3, &#39;ES_0&#39;: 0.0, &#39;P_0&#39;: 0.0 } . traj_euler_full_2 = integrate_euler_full(default_kinetic_params, steps=2e5, initial_conditions=initial_conditions) traj_euler_mm_2 = integrate_euler_MM(default_kinetic_params, steps=2e5, initial_conditions=initial_conditions) ax = traj_euler_full_2.plot.line(title=param_string(**initial_conditions, **default_kinetic_params), color=color(traj_euler_full_2.columns)) traj_euler_mm_2.plot.line(ax=ax, color=color(traj_euler_mm_2.columns), linestyle=&#39;--&#39;) fig_style(ax) . Then P_MM worsens significantly as an estimate of P. . 2.3: Breaking the Michaelis-Menten/Briggs-Haldane Assumptions: &nbsp; &nbsp; &nbsp; Fast Enzyme-Substrate Complex Kinetics . Suppose further that the rates of association and dissociation of enzyme with subtstrate are not substantially faster than those of enzyme and product. . kinetic_params = { &#39;k_on&#39;: 0.05, &#39;k_off&#39;: 1, &#39;k_cat&#39;: 50, &#39;k_uncat&#39;: 0.5 } kinetic_params[&#39;k_ms&#39;] = k_ms(kinetic_params) kinetic_params[&#39;k_mp&#39;] = k_mp(kinetic_params) . traj_euler_full_3 = integrate_euler_full(kinetic_params, initial_conditions=initial_conditions) traj_euler_mm_3 = integrate_euler_MM(kinetic_params, initial_conditions=initial_conditions) ax = traj_euler_full_3.plot.line(title=param_string(**initial_conditions, **kinetic_params), color=color(traj_euler_full_3.columns)) traj_euler_mm_3.plot.line(ax=ax, color=color(traj_euler_mm_3.columns), linestyle=&#39;--&#39;) fig_style(ax) . Then the Michaelis-Menten/Briggs-Haldane kinetics diverge further. . In each of these latter trajectories, the criteria to make the Michaelis-Menten/Briggs-Haldane approximation are violated, leading to poor approximations to the full kinetics. We belabor this point here because in the following, we will seek to infer the parameters of the kinetics, and our inference will fit poorly if we fit to inappropriate kinetic expressions. . 2.4: Comparing Integrators . All of the above trajectories are generated by Euler&#39;s Method, the most intuitive ODE integration technique. Unfortunately, Euler&#39;s Method&#39;s naïvete has drawbacks: . The order of the error is large with respect to the timestep size. | The method is slow, due to the uniform timestep size. | . A variety of faster and more accurate (albeit more complicated) integrators have been proposed, many of which have implementations in scipy&#39;s integrate package. Due to their superior speeds and accuracies, we&#39;ll use these methods during inference. As a sanity check, we compare our basic Euler Method solver to scipy&#39;s: . # define scipy_full and scipy_MM functions (and helpers) to integrate chemical kinetics with scipy from scipy.integrate import solve_ivp def dy_full(t, y, S_0, E_0, ES_0, P_0, k_on, k_off, k_cat, k_uncat, *args): # Y ordered S,E,ES,P dy = np.zeros(4) dy[0] = k_off * y[2] - k_on * y[1] * y[0] dy[1] = k_off * y[2] - k_on * y[1] * y[0] + k_cat * y[2] - k_uncat * y[1] * y[3] dy[2] = k_on * y[1] * y[0] - k_off * y[2] - k_cat * y[2] + k_uncat * y[1] * y[3] dy[3] = k_cat * y[2] - k_uncat * y[1] * y[3] return dy def dy_MM(t, y, S_0, E_0, ES_0, P_0, k_on, k_off, k_cat, k_uncat, k_ms, k_mp): # Y ordered S,P dy = np.zeros(2) dy[1] = ((k_cat * E_0 * y[0]) / k_ms - (k_off * E_0 * y[1]) / k_mp) / (1 + y[0] / k_ms + y[1] / k_mp) dy[0] = -dy[1] return dy def integrate_scipy_full(kinetic_params, initial_conditions=default_initial_conditions, dt=dt, steps=steps): t_span = (0, dt*steps) t_eval = np.around(np.linspace(t_span[0],t_span[1],1001), decimals=5) y0 = list(initial_conditions.values()) try: sol = solve_ivp(dy_full, t_span, y0, args=(*initial_conditions.values(), *kinetic_params.values()), t_eval=t_eval, first_step=dt) return pd.DataFrame(sol.y.T, index=sol.t, columns=[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;]) except: return pd.DataFrame(columns=[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;]) def integrate_scipy_MM(kinetic_params, initial_conditions=default_initial_conditions, dt=dt, steps=steps): t_span = (0, dt*steps) t_eval = np.around(np.linspace(t_span[0],t_span[1],1001), decimals=5) y0 = [initial_conditions[&#39;S_0&#39;], initial_conditions[&#39;P_0&#39;]] try: sol = solve_ivp(dy_MM, t_span, y0, args=(*initial_conditions.values(), *kinetic_params.values()), t_eval=t_eval, first_step=dt) return pd.DataFrame(sol.y.T, index=sol.t, columns=[&#39;S_MM&#39;, &#39;P_MM&#39;]) except: return pd.DataFrame(columns=[&#39;S_MM&#39;, &#39;P_MM&#39;]) . # solve the system with both integrators, for default parameters traj_scipy_full = integrate_scipy_full(default_kinetic_params) ax = traj_scipy_full.plot.line(title=param_string(**default_initial_conditions, **default_kinetic_params), color=color(traj_scipy_full.columns), alpha=0.5) traj_euler_full.plot.line(ax=ax, color=color(traj_euler_full.columns), linestyle=&#39;--&#39;) fig_style(ax) . # solve the system with both integrators, for unusual parameters kinetic_params = { &#39;k_on&#39;: 0.02, &#39;k_off&#39;: 5, &#39;k_cat&#39;: 10, &#39;k_uncat&#39;: 0.00001, } kinetic_params[&#39;k_ms&#39;] = k_ms(kinetic_params) kinetic_params[&#39;k_mp&#39;] = k_mp(kinetic_params) import time start = time.process_time() traj_scipy_full_4 = integrate_scipy_full(kinetic_params, initial_conditions=initial_conditions) scipy_time = time.process_time() - start start = time.process_time() traj_euler_full_4 = integrate_euler_full(kinetic_params, initial_conditions=initial_conditions) euler_time = time.process_time() - start ax = traj_scipy_full_4.plot.line(title=param_string( **initial_conditions,**kinetic_params), color=color(traj_scipy_full_4.columns), alpha=0.5) traj_euler_full_4.plot.line(ax=ax, color=color(traj_euler_full_4.columns), linestyle=&#39;--&#39;) fig_style(ax) . The lack of deviation gives us confidence both integration techniques are accurate. Meanwhile, . f&#39;our naïve code takes {round(euler_time, 2)}s, whereas the optimized scipy code takes {round(scipy_time, 4)}s to generate the same trajectory.&#39; . &#39;our naïve code takes 1.11s, whereas the optimized scipy code takes 0.0103s to generate the same trajectory.&#39; . 3. Inference . We have seen how the trajectory of the chemical system is a function of the kinetic parameters. We would now like to invert that function to recover the kinetic parameters from an observed trajectory. . Suppose we know the initial concentrations of Enzyme E and Substrate S, and we measure the concentration of product P over the course of the reaction, which yields the following dataset: . # plot inverse problem setting measurement_times = np.arange(10+1)/20 observations = traj_scipy_full.loc[measurement_times, &#39;P&#39;] ax = observations.plot.line(marker=&#39;o&#39;, lw=0, color=color([&#39;P&#39;]), legend=True) traj_scipy_full.loc[0, [&#39;E&#39;, &#39;S&#39;]].to_frame().T.plot.line(ax=ax, marker=&#39;o&#39;, lw=0, color=color(traj_scipy_full.columns), legend=True) fig_style(ax) . We will supply noiseless measurements to our inference algorithms. However, our inference procedures will assume noise in the measurements. . Note: If we had measured $ dPdt$ for various (linearly independent) concentrations of $[ mathrm{S}]$, $[ mathrm{P}]$, and $[ mathrm{E}]_0$ (as in an in vitro enzyme assay) we could use a nonlinear regression with the Michaelis-Menten/Briggs-Haldane expression for $ dPdt$. Concretely, supposing we had a set of measurements for the variables in blue, a nonlinear regression would permit us to fit the constants in red: $$ color{blue}{ dPdt} = frac{ frac{ color{red}{ kcat} , color{blue}{[ mathrm{E_T}]} color{blue}{[ mathrm{S}]}} { color{red}{K_{m, mathrm{S}}}} - frac{ color{red}{ koff} , color{blue}{[ mathrm{E_T}]} color{blue}{[ mathrm{P}]}}{ color{red}{K_{m, mathrm{P}}}}} {1+ frac{ color{blue}{[ mathrm{S}]}}{ color{red}{K_{m, mathrm{S}}}} + frac{ color{blue}{[ mathrm{P}]}}{ color{red}{K_{m, mathrm{P}}}}} $$ If we had assumed the reaction were irreversible, the Michaelis-Menten/Briggs-Haldane expression would have simplified to $$ color{blue}{ dPdt} = frac{ color{red}{ kcat} , color{blue}{[ mathrm{E_T}]} color{blue}{[ mathrm{S}]}} { color{red}{K_{m, mathrm{S}}} + color{blue}{[ mathrm{S}]}} $$ Where $ color{red}{ kcat} , color{blue}{[ mathrm{E_T}]}$ is often consolidated as $ color{red}{V_{max}}$. To recap, we take a different approach because: Simultaneous measurements of the activity many enzymes in cells might inform us about $[ mathrm{S}]$, $[ mathrm{P}]$, and perhaps $[ mathrm{E}]$ but not $ dPdt$. We would also presumably not be able to approximate $ dPdt$ via finite differences, due to the relative sparsity of the measurement in time compared to the rates of the reactions. | This approach would produce spurious estimates of the kinetic parameters in cases in which the Quasi-Steady-State Approximation is invalid (see §2.2, §2.3) which may often be the case in vivo. | At the moment, I believe there are no methods for the inverse problem which are not variants of the two methods I will describe, and importantly, no methods which do not iterate a loop, solving the forward problem at each iteration. There are two types of approaches to solving this inverse problem. We will explore the simplest variant of each type. . 3.1 Bayesian Approach: Inference by Sampling . [We assume the reader is familiar with Bayesian Inference in other settings.] . The goal of the Bayesian approach is to determine a posterior over the 4D space spanned by the kinetic parameters. The posterior is the product of the prior and likelihood (up to a constant factor). Thus the Bayesian Inference approach entails defining a prior and a likelihood. . 3.1.1. Prior . If the kinetic parameters of the enzyme under study are not unlike the kinetic parameters of enzymes studied in the past, then the empirical distribution of kinetic parameters of enzymes studied in the past is a good prior for the parameters of this enzyme. . Since databases of observed enzyme kinetic parameters (e.g. BRENDA, SabioRK) are difficult to work with, we&#39;ll use a previously curated set of kinetic parameters from the supplement of The Moderately Efficient Enzyme: Evolutionary and Physicochemical Trends Shaping Enzyme Parameters. . If we knew what sort of enzyme we were studying (which EC class) we could narrow our prior to just those kinetic parameters observed for enzymes of that class. . This database lists $k_{ mathrm{m}}$ and $ kcat$ for both &quot;forwards&quot; and &quot;reverse&quot; reactions with respect to which direction biologists believe is &quot;productive&quot;, from which we can parlay distributions for $ kms$ and $ kcat$ from reactions in the forwards direction, and $ kmp$ and $ koff$ from reverse reactions. . # import kinetic parameter database df = pd.read_excel(&#39;../data/Enzyme_Kinetic_Parameter_Inference/Moderately_Efficient_Enzyme/bi2002289_si_003.xls&#39;, 1)[[&#39;Reaction direction (KEGG)&#39;,&#39;KM (µM)&#39;,&#39;kcat (1/sec)&#39;]] empirical_kms = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == 1, &#39;KM (µM)&#39;].dropna().rename(&#39;k_ms&#39;) empirical_kmp = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == -1, &#39;KM (µM)&#39;].dropna().rename(&#39;k_mp&#39;) empirical_kcat = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == 1, &#39;kcat (1/sec)&#39;].dropna().rename(&#39;k_cat&#39;) empirical_koff = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == -1, &#39;kcat (1/sec)&#39;].dropna().rename(&#39;k_off&#39;) empirical_joint_forward_params = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == 1, [&#39;KM (µM)&#39;,&#39;kcat (1/sec)&#39;]].dropna().rename(columns={&#39;KM (µM)&#39;:&#39;k_ms&#39;, &#39;kcat (1/sec)&#39;:&#39;k_cat&#39;}) empirical_joint_reverse_params = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == -1, [&#39;KM (µM)&#39;,&#39;kcat (1/sec)&#39;]].dropna().rename(columns={&#39;KM (µM)&#39;:&#39;k_mp&#39;, &#39;kcat (1/sec)&#39;:&#39;k_off&#39;}) . # figure styles def fig_style_2(ax): for side in [&quot;right&quot;,&quot;top&quot;,&quot;left&quot;]: ax.spines[side].set_visible(False) ax.get_yaxis().set_visible(False) . # plot km distribution in log-space log_empirical_kms = np.log10(empirical_kms) log_empirical_kmp = np.log10(empirical_kmp) log_kms_normal = scipy.stats.norm(loc=log_empirical_kms.mean(), scale=log_empirical_kms.std()) log_kmp_normal = scipy.stats.norm(loc=log_empirical_kmp.mean(), scale=log_empirical_kmp.std()) ax = log_empirical_kms.plot.hist(bins=500, alpha=0.3, density=1, legend=True) log_empirical_kmp.plot.hist(bins=500, ax=ax, alpha=0.3, density=1, legend=True) ax.set_xlabel(&#39;log₁₀(k_m[µM]) histogram&#39;, weight=&#39;bold&#39;) fig_style_2(ax) # x1 = np.linspace(log_kms_normal.ppf(0.01), log_kms_normal.ppf(0.99), 100) # ax.plot(x1, log_kms_normal.pdf(x1)*ax.get_ylim()[1]*3, &#39;-&#39;, lw=0.6, color=&#39;dodgerblue&#39;) # x2 = np.linspace(log_kmp_normal.ppf(0.01), log_kmp_normal.ppf(0.99), 100) # ax.plot(x2, log_kmp_normal.pdf(x2)*ax.get_ylim()[1]*3, &#39;-&#39;, lw=0.8, color=&#39;peru&#39;) . This plot is surprising: according to this database, enzymes appear to have roughly equal binding affinity for their substrates and products. . # plot kcat distribution in log-space log_empirical_kcat = np.log10(empirical_kcat) log_empirical_koff = np.log10(empirical_koff) log_kcat_normal = scipy.stats.norm(loc=log_empirical_kcat.mean(), scale=log_empirical_kcat.std()) log_koff_normal = scipy.stats.norm(loc=log_empirical_koff.mean(), scale=log_empirical_koff.std()) ax = log_empirical_kcat.plot.hist(bins=500, alpha=0.3, density=1, legend=True) log_empirical_koff.plot.hist(bins=500, ax=ax, alpha=0.3, density=1, legend=True) x1 = np.linspace(log_kcat_normal.ppf(0.01), log_kcat_normal.ppf(0.99), 100) ax.plot(x1, log_kcat_normal.pdf(x1)*ax.get_ylim()[1]*3, &#39;-&#39;, lw=0.4, color=&#39;dodgerblue&#39;) x2 = np.linspace(log_koff_normal.ppf(0.01), log_koff_normal.ppf(0.99), 100) ax.plot(x2, log_koff_normal.pdf(x2)*ax.get_ylim()[1]*3, &#39;-&#39;, lw=0.6, color=&#39;peru&#39;) ax.set_xlabel(&#39;log₁₀(k_cat[1/s]) histogram&#39;, weight=&#39;bold&#39;) fig_style_2(ax) . On the other hand, they have a fairly strong preference for catalyzing the reaction biologists think of as forwards (~10x). . Since these empirical distributions over $ kms$ and $ kcat$ in the forwards direction and $ kmp$ and $ koff$ in the reverse direction look sufficiently like normals in log space, so we&#39;ll treat them as lognormals. However, we would like our inference procedure to estimate the semantic parameters $ kon$, $ koff$, $ kcat$, and $ kuncat$. We can rearrange the expressions for $ kms$ and $ kmp$ to get expressions for the two parameters we&#39;re missing: . $$ kon = frac{ koff + kcat}{ kms} quad mathrm{and} quad kuncat = frac{ koff + kcat}{ kmp}$$ . Conveniently, the ratio of lognormal variables $ frac{X_1}{X_2}$ is also lognormal with $ mu_{1/2} = mu_1 - mu_2$ and $ sigma^2_{1/2} = sigma^2_1 + sigma^2_2 - sigma_{x_1, x_2}$. In order to use that fact, we say the sum of the random variables $ koff + kcat$ is also log-normally distributed. We compute its mean and variance empirically. . kcat_plus_koff = pd.Series(np.repeat(empirical_kcat.values, len(empirical_koff)) + np.tile(empirical_koff.values, len(empirical_kcat))) log_kcat_plus_koff_mean = np.log10(kcat_plus_koff).mean() log_kcat_plus_koff_var = np.log10(kcat_plus_koff).var() . This permits us to produce empirical distributions for $ kon$ and $ kuncat$, . log_kon_normal = scipy.stats.norm(loc=log_kcat_plus_koff_mean-log_empirical_kms.mean(), scale=sqrt(log_kcat_plus_koff_var+log_empirical_kms.var())) log_kuncat_normal = scipy.stats.norm(loc=log_kcat_plus_koff_mean-log_empirical_kmp.mean(), scale=sqrt(log_kcat_plus_koff_var+log_empirical_kmp.var())) . which, along with our empirical distributions for $ koff$ and $ kcat$, define a prior over the 4 kinetic parameters we wish to infer. . We might ask whether these are correlated lognormals . pp = sns.pairplot(np.log10(empirical_joint_forward_params), kind=&quot;kde&quot;, plot_kws={&#39;linewidths&#39;:0.5, &#39;color&#39;:&#39;darkolivegreen&#39;}) k_ms_univariate_density = pp.diag_axes[0].get_children()[0] k_ms_univariate_density.set_edgecolor(c[&#39;k_ms&#39;]) k_ms_univariate_density.set_facecolor(hex_to_rgb(c[&#39;k_ms&#39;]) + [0.1]) k_cat_univariate_density = pp.diag_axes[1].get_children()[0] k_cat_univariate_density.set_edgecolor(c[&#39;k_cat&#39;]) k_cat_univariate_density.set_facecolor(hex_to_rgb(c[&#39;k_cat&#39;]) + [0.1]) resize_fig(400, 400) . pp = sns.pairplot(np.log10(empirical_joint_reverse_params), kind=&quot;kde&quot;, plot_kws={&#39;linewidths&#39;:0.5, &#39;color&#39;:&#39;grey&#39;}) k_mp_univariate_density = pp.diag_axes[0].get_children()[0] k_mp_univariate_density.set_edgecolor(c[&#39;k_mp&#39;]) k_mp_univariate_density.set_facecolor(hex_to_rgb(c[&#39;k_mp&#39;]) + [0.1]) k_off_univariate_density = pp.diag_axes[1].get_children()[0] k_off_univariate_density.set_edgecolor(c[&#39;k_off&#39;]) k_off_univariate_density.set_facecolor(hex_to_rgb(c[&#39;k_off&#39;]) + [0.1]) resize_fig(400, 400) . Not enough to include covariances in the prior. We set the prior covariance to be a diagonal matrix: . prior_cov = np.diag([log_kon_normal.var(), log_koff_normal.var(), log_kcat_normal.var(), log_kuncat_normal.var()]) . def prior_pdf(k_on=None, k_off=None, k_cat=None, k_uncat=None, **kwargs): return ( log_kon_normal.pdf(k_on) * log_koff_normal.pdf(k_off) * log_kcat_normal.pdf(k_cat) * log_kuncat_normal.pdf(k_uncat)) def prior_logpdf(k_on=None, k_off=None, k_cat=None, k_uncat=None, **kwargs): return ( log_kon_normal.logpdf(k_on) + log_koff_normal.logpdf(k_off) + log_kcat_normal.logpdf(k_cat) + log_kuncat_normal.logpdf(k_uncat)) def sample_prior(): # returns [k_on, k_off, k_cat, k_uncat] return { &#39;k_on&#39;: log_kon_normal.rvs(), &#39;k_off&#39;: log_koff_normal.rvs(), &#39;k_cat&#39;: log_kcat_normal.rvs(), &#39;k_uncat&#39;: log_kuncat_normal.rvs()} . Now that we have a prior, let&#39;s examine where the default parameters introduced in §2.1 land in this distribution. We had claimed they were &quot;typical&quot;. . # plot log-space kinetic parameter distributions, with default parameters presented previously overlaid fig, axs = plt.subplots(2,2,constrained_layout=True) def plot_distrib(distrib, ax, title, param): ax.set_xlim(-7,7) ax.set_ylim(0,0.35) x = np.linspace(distrib.ppf(0.001), distrib.ppf(0.999), 100) y = distrib.pdf(x) color = c[param] ax.plot(x, y, &#39;-&#39;, lw=0.7, color=color) ax.fill_between(x, 0, y, color=color, alpha=0.1) ax.axvline(np.log10(default_kinetic_params[param]), 0, 1, linestyle=&#39;--&#39;, color=color) ax.xaxis.set_ticks(np.arange(-7, 7.1, 2)) ax.set_xlabel(title, weight=&#39;bold&#39;) fig_style_2(ax) plot_distrib(log_kon_normal, axs[0][0], &#39;log₁₀(k_on[µM])&#39;, &#39;k_on&#39;) plot_distrib(log_koff_normal, axs[0][1], &#39;log₁₀(k_off[1/s])&#39;, &#39;k_off&#39;) plot_distrib(log_kuncat_normal, axs[1][0], &#39;log₁₀(k_uncat[µM])&#39;, &#39;k_uncat&#39;) plot_distrib(log_kcat_normal, axs[1][1], &#39;log₁₀(k_cat[1/s])&#39;, &#39;k_cat&#39;) . 3.1.2. Likelihood . We need to define a likelihood $p(D| theta)$ which measures the probability of producing the observed data given settings of the kinetic parameters $ theta = { kon, koff, kcat, kuncat }$. Our data $D = { color{00008b}{ [ mathrm{P}]_t } , color{black}{ ; t in 0...0.5 }}$ are an observed trajectory of concentrations of reaction product P. Each setting of the kinetic parameters corresponds to a trajectory of concentrations of P (via a numerical integration). Intuitively, parameter sets which result in trajectories very near the observed trajectory are more likely. Therefore, our likelihood should measure the distance between the observed $ { color{00008b}{ [ mathrm{P}]_t } color{black}{ } }$ and predicted $ { color{blue}{ [ mathrm{P}]_t } color{black}{ } }$. . How far should the predicted trajectory be allowed to stray from the measured $ { color{00008b}{ [ mathrm{P}]_t } color{black}{ } }$? The likelihood is really our statement about the presumed noise in our measurements. If we believe our measurements to be noiseless, then our likelihood should concentrate tightly around our measurements (a dirac $ delta$ in the limit), and we would only admit kinetic parameters that interpolate the observed $ { color{00008b}{ [ mathrm{P}]_t } color{black}{ } }$ almost exactly. In reality, no measurement is noiseless, so we propose the following noise model: . Supposing the detection of each molecule of P is an independent binary random variable with error rate $ sigma$ then random variable $ color{red}{[ mathrm{P}]_t}$ is gaussian-distributed $ sim mathcal{N}( color{00008b}{[ mathrm{P}]_t} color{black}, sigma sqrt{ color{00008b}{[ mathrm{P}]_t }} color{black} )$. The variance of the gaussian grows as the square root of the mean, via a Central Limit Theorem argument. We can represent this noise model (and consequently, likelihood) visually as: . σ = 5 # arbitrary magic number represents detection noise level . # plot intuition for likelihood definition plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] ax = observations.rename(&#39;observations&#39;).plot.line(marker=&#39;.&#39;, lw=0, color=color([&#39;P&#39;]), legend=True, markersize=2) domain = np.linspace(0,10000,1000) for x, y in observations.iloc[1:].items(): distrib = scipy.stats.norm(loc=y, scale=np.sqrt(y)*σ) ax.fill_betweenx(domain, [x]*1000, x2=x+distrib.pdf(domain)/distrib.pdf(y)/150, color=color([&#39;P&#39;]), alpha=0.2) fig_style(ax) . Concretely, the likelihood is the product distribution of each of the gaussian marginals centered around the measurements. These form a multivariate normal, diagonal since we neglect to add covariances. . $$p(D| theta) = displaystyle prod_{t=0}^{0.5} p_t( color{blue}{[ mathrm{P}]_t} color{black}) textrm{ where } p_t textrm{ is the density of } color{red}{[ mathrm{P}]_t} color{black} sim mathcal{N}( color{00008b}{[ mathrm{P}]_t} color{black}, sigma sqrt{ color{00008b}{[ mathrm{P}]_t }} color{black} )$$ . Which leaves us with a &quot;hyperparameter&quot; $ sigma$. . likelihood_dist = multivariate_normal(mean=observations.values[1:], cov=σ * np.diag(sqrt(observations.values[1:]))) def likelihood_logpdf(ut): return likelihood_dist.logpdf(ut) . 3.1.3. Metropolis-Hastings . # define simulated_measurements() def exp_params(log_kinetic_params): return {name: 10**val for name, val in log_kinetic_params.items()} def simulate_measurements(kinetic_params): u = integrate_scipy_full(kinetic_params) return (u.loc[measurement_times, &#39;P&#39;].ravel()[1:] if len(u) &gt; 0 else np.zeros(10)) . We can now evaluate the prior $p( theta)$ and the likelihood $p(D| theta)$ of kinetic parameters $ theta = { kon, koff, kcat, kuncat }$. Those two distributions permit us to elaborate an Markov Chain Monte Carlo (MCMC) routine to sample from the posterior $p( theta|D) propto p(D| theta) cdot p( theta)$. The algorithm is as follows: . Repeat: . Draw kinetic parameters from the proposal distribution. | Integrate the system with the proposed kinetic parameters. | Evaluate the likelihood of the trajectory generated in step 2. | Accept/Reject the proposal by a Metropolis-Hastings criterion. | Append the current kinetic parameters to the Markov Chain. | Construct a proposal distribution around the current kinetic parameters. | Since the likelihood assigns most of the probability mass to a fairly narrow region of parameter space, most parameter sets have extremely low probability. In order to preserve some numerical stability, we log-transform the typical Metropolis-Hastings expressions. So typically $π_t = mathrm{likelihood _pdf}(u_t) cdot mathrm{prior _pdf}(θ_t)$ and the acceptance criterion is $ frac{π_{t+1}}{π_t} &gt; mathrm{rand}([0,1])$. In log space, the acceptance criterion becomes: $ log(π_{t+1}) - log(π_t) &gt; log( mathrm{rand}([0,1]))$ with $ log(π_t) = mathrm{likelihood _logpdf}(u_t) + mathrm{prior _logpdf}(θ_t)$. . def MH_MCMC(chain_length=1e3): θt = sample_prior() ut = simulate_measurements(exp_params(θt)) πt = likelihood_logpdf(ut) + prior_logpdf(**θt) if all(ut == 0): return MH_MCMC(chain_length) cov = np.eye(4) * 5e-4 i = 0 accept_ratio = 0 chain = [] samples = [] while i &lt; chain_length: θtp1 = proposal(θt, cov) utp1 = simulated_measurements(θtp1) πtp1 = likelihood_logpdf(utp1) + prior_logpdf(**θtp1) if πtp1 - πt &gt; np.log(np.random.rand()): θt, ut, πt = θtp1, utp1, πtp1 accept_ratio += 1 chain.append(θt) samples.append(ut) i += 1 if i % 100 == 0 and i &gt; 300: # cov = pd.DataFrame(chain[100:]).cov() print(i, end=&#39; r&#39;) chain = pd.DataFrame(chain) samples = pd.DataFrame(np.hstack((np.zeros((len(chain), 1)), samples)), columns=observations.index) accept_ratio = accept_ratio/chain_length return chain, samples, accept_ratio . Our proposal density for the time being can be a simple isotropic gaussian around the current parameters. . def proposal(θt, cov): μ = [θt[&#39;k_on&#39;], θt[&#39;k_off&#39;], θt[&#39;k_cat&#39;], θt[&#39;k_uncat&#39;]] θtp1 = dict(zip([&#39;k_on&#39;, &#39;k_off&#39;, &#39;k_cat&#39;, &#39;k_uncat&#39;], np.random.multivariate_normal(μ, cov))) return θtp1 . Now let&#39;s put it into practice: . chain_length = 1e3 chain, samples, accept_ratio = MH_MCMC(chain_length=chain_length) print(&#39;accept_ratio:&#39;, accept_ratio) . accept_ratio: 0.151 . def fig_style_3(ax): for side in [&quot;right&quot;,&quot;top&quot;]: ax.spines[side].set_visible(False) ax.set_xlabel(&#39;chain&#39;, weight=&#39;bold&#39;) ax.set_ylabel(&#39;log parameter values&#39;, weight=&#39;bold&#39;) . def plot_chain(chain, ax=None): if ax is None: fig, ax = plt.subplots() chain.plot.line(xlim=(0,len(chain)), color=[c[param_name] for param_name in chain.columns], ax=ax) for param_name in chain.columns: param_value = default_kinetic_params[param_name] ax.axhline(np.log10(param_value), lw=0.5, color=c[param_name], linestyle=&#39;--&#39;) ax.fill_between(np.arange(len(chain)), chain[param_name], np.repeat(np.log10(param_value), len(chain)), color=c[param_name], alpha=0.05) fig_style_3(ax) . plot_chain(chain) . sns.pairplot(chain, kind=&quot;kde&quot;) resize_fig(600, 600) . def plot_samples(samples, ax=None): if ax is None: fig, ax = plt.subplots() observations.plot.line(marker=&#39;o&#39;, lw=0, color=c[&#39;P&#39;], ylim=(-300, 10800), ax=ax, legend=True) samples.T.plot.line(colormap=plt.get_cmap(&#39;plasma&#39;), alpha=0.1, ax=ax, legend=False, zorder=1) fig_style(ax) . plot_samples(samples) . def MCMC_run(): chain, samples, accept_ratio = MH_MCMC(chain_length=chain_length) fig, axs = plt.subplots(1, 2) plot_chain(chain, ax=axs[0]) plot_samples(samples, ax=axs[1]) print(&#39;accept_ratio:&#39;, accept_ratio) . MCMC_run() . accept_ratio: 0.079 . MCMC_run() . accept_ratio: 0.101 . MCMC_run() . accept_ratio: 0.118 . A few things pop out from the above chains: . It appears to be possible to closely fit the observed data with very different parameter sets than the ones used to generate the observed trajectory. | It appears that our chain is finding local maxima in the posterior and struggling to escape. . Note: Some of the above trajectories appear non-smooth. That&#8217;s only a consequence of the fact we&#8217;re visualizing a coarse sampling of those trajectories -- the underlying trajectories are sampled much more densely by scipy&#8217;s integrator. | # define euler_full(), which integrates the full kinetics with Euler&#39;s Method, and returns a trajectory def euler_full_modified(dt, steps, E_0=None, S_0=None, k_on=None, k_off=None, k_cat=None, k_uncat=None, k_ms=None, k_mp=None): S = S_0 E = E_0 ES = 0 P = 0 traj = [] for _ in range(int(steps)): k_off_ES = k_off * ES k_on_E_S = k_on * E * S k_cat_ES = k_cat * ES k_uncat_E_P = k_uncat * E * P traj.append([S, E, ES, P, k_off_ES, k_on_E_S, k_cat_ES, k_uncat_E_P]) dS = k_off_ES - k_on_E_S dE = k_off_ES - k_on_E_S + k_cat_ES - k_uncat_E_P dES = k_on_E_S - k_off_ES - k_cat_ES + k_uncat_E_P dP = k_cat_ES - k_uncat_E_P S += dS * dt E += dE * dt ES += dES * dt P += dP * dt return pd.DataFrame(traj, columns=[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;, &#39;k_off_ES&#39;, &#39;k_on_E_S&#39;, &#39;k_cat_ES&#39;, &#39;k_uncat_E_P&#39;], index=np.around(np.linspace(0, dt*steps, int(steps)), 6)) . modified_traj_full = euler_full_modified(dt, steps, **default_kinetic_params, **default_initial_conditions) . ax = modified_traj_full[[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;]].plot.line(title=param_string(**default_initial_conditions, **default_kinetic_params), color=color(default_traj_full.columns)) fig_style(ax) . ax = modified_traj_full.iloc[500:][[&#39;k_off_ES&#39;, &#39;k_on_E_S&#39;, &#39;k_cat_ES&#39;, &#39;k_uncat_E_P&#39;]].plot.line(color=color(default_traj_full.columns)) None . $$ begin{aligned} frac{d[ mathrm{S}]}{dt} &amp;= k_{ mathrm{off}}[ mathrm{ES}] - k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] frac{d[ mathrm{E}]}{dt} &amp;= k_{ mathrm{off}}[ mathrm{ES}] - k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] + k_{ mathrm{cat}}[ mathrm{ES}] - k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] frac{d[ mathrm{ES}]}{dt} &amp;= - k_{ mathrm{off}}[ mathrm{ES}] + k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] - k_{ mathrm{cat}}[ mathrm{ES}] + k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] frac{d[ mathrm{P}]}{dt} &amp;= k_{ mathrm{cat}}[ mathrm{ES}] - k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] end{aligned}$$ 3.2 Frequentist Approach: Inference by Optimization . In the previous section, our approach was to wander around parameter space, biasing our random walk towards regions of the parameter space where both the prior probability and likelihood were greater. After a certain number of samples, the samples from our random walk constitute an (asymptotically exact) estimate of the entire probability distribution over the values of the kinetic parameters. . An alternative approach begins with another premise: . Suppose we want to incorporate no prior knowledge, and let the timeseries data alone govern our determination of our enzyme&#39;s kinetic parameters. | Suppose as well that instead of searching for a distribution of plausible parameters, we&#39;re only interested in finding the single most likely set of parameters. | . These two choices recast the inference task as an optimization problem. . Optimization problems require an objective, such as minimizing a loss (or cost) function. Let&#39;s use the conventional squared error between our trajectory $u$ and the data $d$: $G(u(t, theta)) = sum_t ( | d(t) - u(t, theta) |_2)^2$ illustrated below as the loss function we&#39;ll minimize. . def loss(u): return np.linalg.norm(observations - u.loc[observations.index, &#39;P&#39;])**2 . # plot intuition for squared loss definition plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] ax = observations.rename(&#39;observations d(t)&#39;).plot.line(marker=&#39;o&#39;, lw=0, color=color([&#39;P&#39;]), legend=True, markersize=5, ylim=(-5e2,1e4)) (observations * 1.1).rename(&#39;simulation u(t)&#39;).plot.line(marker=&#39;o&#39;, lw=0.4, color=plt.get_cmap(&#39;plasma&#39;)(0.8), legend=True, markersize=3) # total_err = 0 for x, y in observations.iloc[1:].items(): rect = patches.Rectangle((x, y), y*0.1/1e4*0.225, y*0.1, linewidth=1, edgecolor=&#39;r&#39;, facecolor=&#39;mistyrose&#39;, lw=0.3) ax.add_patch(rect) fig_style(ax) . 3.2.1 Forward Sensitivities . In order to optimize our parameters $ theta$ with respect to to our loss function $G$, we need a means to evaluate the gradient of the loss with respect to the parameters. Naively: . $$ frac{dG(u(t, theta))}{d theta} = frac{d}{d theta} sum_t( | d(t) - u(t, theta) |_2)^2 = sum_t left[ 2(d(t) - u(t, theta)) frac{du(t, theta)}{d theta} right]$$However, the quantity $ frac{du(t, theta)}{d theta}$ is not immediately available. We can derive it as follows: . Our original differential equation is $ frac{du(t, theta)}{dt} = f(u(t, theta), theta)$. If we take $ frac{ partial}{ partial theta} left[ frac{du(t, theta)}{dt} right] = frac{ partial}{ partial theta} left[ f(u(t, theta), theta) right]$, we can rearrange as $ frac{d}{dt} left[ frac{ partial u(t, theta)}{ partial theta} right] = frac{ partial}{ partial theta} left[ f(u(t, theta), theta) right]$ and then integrate over $t$ for . $$ int_{t_0}^T frac{d}{dt} left[ frac{ partial u(t, theta)}{ partial theta} right]dt = int_{t_0}^T frac{ partial}{ partial theta} left[ f(u(t, theta), theta) right]dt = int_{t_0}^T left[ frac{ partial f}{ partial u} Big|_{u(t, theta), theta} frac{ partial u}{ partial theta} Big|_t + frac{ partial f}{ partial theta} Big|_{u(t, theta), theta} right] dt$$Which is exactly $ frac{du(t, theta)}{d theta}$. Surprisingly, what we&#39;ve done is define an ODE whose solution (integral) is the gradient. This ODE is usually called the forward sensitivity ODE. We can solve (integrate) both the original ODE and the sensitivity ODE forwards in time together. . But first, we need to understand the constituent expressions: $ frac{ partial f}{ partial u} Big|_{u(t, theta), theta}$ , $ frac{ partial u}{ partial theta} Big|_t$ and $ frac{ partial f}{ partial theta} Big|_{u(t, theta), theta}$ . Recall, . $$ frac{du}{dt} = frac{d}{dt} begin{bmatrix}[ mathrm{S}] [ mathrm{E}] [ mathrm{ES}] [ mathrm{P}] end{bmatrix} = begin{bmatrix} k_{ mathrm{off}}[ mathrm{ES}] - k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] k_{ mathrm{off}}[ mathrm{ES}] - k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] + k_{ mathrm{cat}}[ mathrm{ES}] - k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] - k_{ mathrm{off}}[ mathrm{ES}] + k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] - k_{ mathrm{cat}}[ mathrm{ES}] + k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] k_{ mathrm{cat}}[ mathrm{ES}] - k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] end{bmatrix} = f(u(t, theta), theta)$$$ frac{ partial f}{ partial u} Big|_{u(t, theta), theta}$ is the derivative of the derivative with respect to the state. Since both are 4D, this is a 4x4 Jacobian: . $$ frac{df}{du} = begin{bmatrix} frac{df}{[ mathrm{S}]} &amp; frac{df}{[ mathrm{E}]} &amp; frac{df}{[ mathrm{ES}]} &amp; frac{df}{[ mathrm{P}]} end{bmatrix} = begin{bmatrix} -k_{ mathrm{on}}[ mathrm{E}] &amp; -k_{ mathrm{on}}[ mathrm{S}] &amp; k_{ mathrm{off}} &amp; 0 -k_{ mathrm{on}}[ mathrm{E}] &amp; -k_{ mathrm{on}}[ mathrm{S}] - k_{ mathrm{uncat}}[ mathrm{P}] &amp; k_{ mathrm{off}} + k_{ mathrm{cat}} &amp; -k_{ mathrm{uncat}} k_{ mathrm{on}}[ mathrm{E}] &amp; k_{ mathrm{on}}[ mathrm{S}] + k_{ mathrm{uncat}}[ mathrm{P}] &amp; -k_{ mathrm{off}} - k_{ mathrm{cat}} &amp; k_{ mathrm{uncat}} 0 &amp; -k_{ mathrm{uncat}}[ mathrm{P}] &amp; k_{ mathrm{cat}} &amp; -k_{ mathrm{uncat}} end{bmatrix}$$ # define helper functions for euler_full_sensitivities() def f_u_Jacobian(S, E, ES, P, k_on, k_off, k_cat, k_uncat): return np.array([ [-k_on * E, -k_on * S, k_off, 0], [-k_on * E, -k_on * S - k_uncat * P, k_off + k_cat, -k_uncat], [k_on * E, k_on * S + k_uncat * P, -k_off - k_cat, k_uncat], [0, -k_uncat * P, k_cat, -k_uncat] ]) . $ frac{ partial f}{ partial theta} Big|_{u(t, theta), theta}$ is the derivative of the derivative with respect to one of the parameters. . $$ frac{ partial f}{ partial k_{ mathrm{on}}} = begin{bmatrix} -[ mathrm{E}][ mathrm{S}] -[ mathrm{E}][ mathrm{S}] [ mathrm{E}][ mathrm{S}] 0 end{bmatrix}, qquad frac{ partial f}{ partial k_{ mathrm{off}}} = begin{bmatrix} [ mathrm{ES}] [ mathrm{ES}] -[ mathrm{ES}] 0 end{bmatrix}, qquad frac{ partial f}{ partial k_{ mathrm{cat}}} = begin{bmatrix} 0 [ mathrm{ES}] -[ mathrm{ES}] [ mathrm{ES}] end{bmatrix}, qquad frac{ partial f}{ partial k_{ mathrm{uncat}}} = begin{bmatrix} 0 -[ mathrm{E}][ mathrm{P}] [ mathrm{E}][ mathrm{P}] -[ mathrm{E}][ mathrm{P}] end{bmatrix}, qquad $$ # define helper functions for euler_full_sensitivities() def f_k_on(S, E, ES, P): return np.array([[-E*S, -E*S, E*S, 0]]).T def f_k_off(S, E, ES, P): return np.array([[ES, ES, -ES, 0]]).T def f_k_cat(S, E, ES, P): return np.array([[0, ES, -ES, ES]]).T def f_k_uncat(S, E, ES, P): return np.array([[0, -E*P, E*P, -E*P]]).T cols = [v+u for u in [&#39;&#39;, &#39;_k_on&#39;, &#39;_k_off&#39;, &#39;_k_cat&#39;, &#39;_k_uncat&#39;] for v in [&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;]] . $ frac{ partial u}{ partial theta} Big|_t$ is the variable of integration, which means we only need to define a boundary condition for it, in this case, an initial value: . $$ frac{ partial u}{ partial theta} Big|_{t_0} = frac{ partial}{ partial theta} u(0, theta) $$ . But since in our case $u(0, theta) = u(0) = 0$ does not depend on $ theta$, $ frac{ partial u}{ partial theta} Big|_{t_0} = 0$. . Now we&#39;re ready to augment our original Euler method to compute both $ int_{t_0}^T frac{du(t, theta)}{dt} dt$ as before and add $ int_{t_0}^T frac{ partial}{ partial theta} left[ f(u(t, theta), theta) right] dt$. . # define euler_full_sensitivities(), which integrates the full kinetics and sensitivity ODE with Euler&#39;s Method def integrate_euler_full_sensitivities(kinetic_params, dt=dt, steps=steps, initial_conditions=default_initial_conditions): k_on, k_off, k_cat, k_uncat, k_ms, k_mp = kinetic_params.values() S, E, ES, P = initial_conditions.values() u_k_on = np.zeros((4,1)) u_k_off = np.zeros((4,1)) u_k_cat = np.zeros((4,1)) u_k_uncat = np.zeros((4,1)) traj = [[S, E, ES, P, *u_k_on.flatten(), *u_k_off.flatten(), *u_k_cat.flatten(), *u_k_uncat.flatten()]] for i in range(int(steps)): S += (k_off * ES - k_on * E * S) * dt E += (k_off * ES - k_on * E * S + k_cat * ES - k_uncat * E * P) * dt ES += (k_on * E * S - k_off * ES - k_cat * ES + k_uncat * E * P) * dt P += (k_cat * ES - k_uncat * E * P) * dt f_u = f_u_Jacobian(S, E, ES, P, k_on, k_off, k_cat, k_uncat) u_k_on += (f_u @ u_k_on + f_k_on(S, E, ES, P)) * dt u_k_off += (f_u @ u_k_off + f_k_off(S, E, ES, P)) * dt u_k_cat += (f_u @ u_k_cat + f_k_cat(S, E, ES, P)) * dt u_k_uncat += (f_u @ u_k_uncat + f_k_uncat(S, E, ES, P)) * dt traj.append([S, E, ES, P, *u_k_on.T[0].copy(), *u_k_off.T[0].copy(), *u_k_cat.T[0].copy(), *u_k_uncat.T[0].copy()]) return pd.DataFrame(traj, columns=cols, index=np.around(np.linspace(0, dt*steps, int(steps)+1), 6)) . start = time.process_time() traj_euler_full_sensitivities = integrate_euler_full_sensitivities(default_kinetic_params) euler_time = time.process_time() - start ODE_columns = traj_euler_full_sensitivities.columns[:4] sensitivity_columns = traj_euler_full_sensitivities.columns[4:] P_sensitivity_columns = traj_euler_full_sensitivities.columns[7::4] fig, axs = plt.subplots(1, 2) traj_euler_full_sensitivities[ODE_columns].plot.line(ax=axs[0], color=color(ODE_columns)) traj_euler_full_sensitivities[sensitivity_columns].plot.line(ax=axs[1]) fig_style(axs[0]) fig_style(axs[1]) . Unfortunately, as before, our simple-minded python code, although conceptually helpful, is too slow to use repeatedly inside a loop. Let&#39;s once again re-structure this code for scipy. . # define scipy_full_sensitivities (and helpers), which integrates the full kinetics and sensitivities with scipy def dy_full_sensitivities(t, y, E_0, S_0, ES_0, P_0, k_on, k_off, k_cat, k_uncat, *args): # Y ordered S,E,ES,P dy = np.zeros(20) dy[0] = k_off * y[2] - k_on * y[1] * y[0] dy[1] = k_off * y[2] - k_on * y[1] * y[0] + k_cat * y[2] - k_uncat * y[1] * y[3] dy[2] = k_on * y[1] * y[0] - k_off * y[2] - k_cat * y[2] + k_uncat * y[1] * y[3] dy[3] = k_cat * y[2] - k_uncat * y[1] * y[3] f_u = f_u_Jacobian(*y[0:4], k_on, k_off, k_cat, k_uncat) dy[4:8] = np.dot(f_u, y[4:8]) + f_k_on(*y[0:4]).T dy[8:12] = np.dot(f_u, y[8:12]) + f_k_off(*y[0:4]).T dy[12:16] = np.dot(f_u, y[12:16]) + f_k_cat(*y[0:4]).T dy[16:20] = np.dot(f_u, y[16:20]) + f_k_uncat(*y[0:4]).T return dy def integrate_scipy_full_sensitivities(kinetic_params, initial_conditions=default_initial_conditions, dt=dt, steps=steps): t_span = (0, dt*steps) t_eval = np.around(np.linspace(t_span[0],t_span[1],1001), decimals=5) y0 = list(initial_conditions.values()) + [0]*16 try: sol = solve_ivp(dy_full_sensitivities, t_span, y0, args=(*initial_conditions.values(), *kinetic_params.values()), t_eval=t_eval, first_step=dt, method=&#39;LSODA&#39;) return pd.DataFrame(sol.y.T, index=sol.t, columns=cols) except: return pd.DataFrame(columns=cols) . # benchmark our naive code against scipy&#39;s integrator for the sensitivity equations start = time.process_time() traj_scipy_full_sensitivities = integrate_scipy_full_sensitivities(default_kinetic_params) scipy_time = time.process_time() - start ax = traj_euler_full_sensitivities[sensitivity_columns].plot.line(color=&#39;r&#39;, alpha=0.5, legend=False) traj_scipy_full_sensitivities[sensitivity_columns].plot.line(ax=ax, color=&#39;r&#39;, linestyle=&#39;--&#39;, legend=False) f&#39;our naïve code takes {round(euler_time, 2)}s, whereas the optimized scipy code takes {round(scipy_time, 4)}s to generate the same trajectory.&#39; . &#39;our naïve code takes 22.39s, whereas the optimized scipy code takes 0.4221s to generate the same trajectory.&#39; . Recall, computing the sensitivity of the solution with respect to the parameters $ frac{du(t, theta)}{d theta}$ was in service of computing the gradient of our loss function with respect to the parameters: . $$ frac{dG(u(t, theta))}{d theta} = sum_t left[ 2(d(t) - u(t, theta)) frac{du(t, theta)}{d theta} right]$$Now, since we set up this problem such that we only observe $ { color{00008b}{ [ mathrm{P}]_t } color{black}{ } }$, we are only able to compare the integrated kinetics of $ { color{blue}{ [ mathrm{P}]_t } color{black}{ } }$ and so our gradient expression becomes: . $$ frac{dG(u(t, theta))}{d theta} = sum_t left[ 2( color{00008b}{ [ mathrm{P}]_t } color{black}{ - color{blue}{ [ mathrm{P}]_t } color{black}{ ) frac{d color{blue}{ [ mathrm{P}]_t }}{d theta} }} right]$$ # define gradient_of_loss() which returns the gradient of the loss with respect to each parameter def gradient_of_loss(integrated_system_and_sensitivities): diff = 2*(np.abs(observations - integrated_system_and_sensitivities.loc[observations.index, &#39;P&#39;])) P_k = integrated_system_and_sensitivities.loc[observations.index, P_sensitivity_columns] grad = P_k.multiply(diff, axis=&#39;rows&#39;).sum() grad.index = grad.index.str.lstrip(&#39;P_&#39;) normd_grad = grad / np.linalg.norm(grad) return normd_grad.to_dict() . traj_scipy_sensitivities[P_sensitivity_columns].plot.line() . &lt;AxesSubplot:&gt; . We notice right away the scale of some of these sensitivities reaches $O(10^6)$. That may be because the parameters span many orders of magnitude, so the sentivities of the system to small perturbations in some parameters may be much greater than others. . We&#39;ve set ourselves the task of optimizing the 4 parameters in the Michaelis-Menten/Briggs-Haldane kinetics ODE to minimize the squared error with respect to an observed timeseries. In order to intialize this optimization routine, we need somewhere to start from. Let&#39;s use the means of the prior distributions (from §3.1.1) for each parameter as a starting point. . θ_0 = {&#39;k_on&#39;: 10**log_kon_normal.mean(), &#39;k_off&#39;: 10**log_koff_normal.mean(), &#39;k_cat&#39;: 10**log_kcat_normal.mean(), &#39;k_uncat&#39;: 10**log_kuncat_normal.mean()} . Our gradient descent routine iterates a loop: . Integrate the system ODE and sensitivity ODE with the current parameters. | Compute the gradient of the loss with the current parameters. | Update the parameters with a gradient step | Optimization_Record = namedtuple(&#39;Optimization_Record&#39;, [&#39;θ&#39;, &#39;u&#39;, &#39;loss&#39;, &#39;G_θ&#39;]) def optimize_by_gradient_descent(θ_0): θ_record = [] u_record = [] loss_record = [] grad_record = [] θ_t = dict(θ_0) α = 1 prev_loss = np.inf print(θ_t) while α &gt;= 1e-9: u = integrate_scipy_full_sensitivities(θ_t) G_θ = gradient_of_loss(u) G_θ_norm = np.linalg.norm(list(G_θ.values())) curr_loss = loss(u) if curr_loss &gt; prev_loss: α = α/np.sqrt(10) θ_t = θ_record[-1] continue θ_record.append(θ_t) u_record.append(u) loss_record.append(curr_loss) grad_record.append(G_θ) θ_t = {k: max(θ_t[k] + (α * G_θ[k] / G_θ_norm), 0) for k in θ_t.keys()} prev_loss = curr_loss print(&#39; t&#39;, θ_t, end=&#39; t t t r&#39;) θ_record = pd.DataFrame(θ_record) loss_record = pd.Series(loss_record) grad_record = pd.DataFrame(grad_record) print(&#39; t&#39;, θ_t, end=&#39; t t t r&#39;) return Optimization_Record(θ=θ_record, u=u_record, loss=loss_record, G_θ=grad_record) . optimization_record = optimize_by_gradient_descent(θ_0) . {&#39;k_on&#39;: 1.0489415890738956, &#39;k_off&#39;: 9.301874926576737, &#39;k_cat&#39;: 26.789395694394962, &#39;k_uncat&#39;: 0.8975846605968467} {&#39;k_on&#39;: 1.70042263313856, &#39;k_off&#39;: 9.268106147314615, &#39;k_cat&#39;: 26.849749195060024, &#39;k_uncat&#39;: 0.1420784900535258} . # plot result of optimization, starting from mean parameter values plt.rcParams[&#39;figure.figsize&#39;] = [12, 4] fig, axs = plt.subplots(1, 2) plot_samples(pd.concat([df[&#39;P&#39;] for df in optimization_record.u], axis=1).T, ax=axs[0]) optimization_record.loss.plot.line(logy=True, ax=axs[1]) axs[1].set_xticks(optimization_record.loss.index[::2]) None . # define def prior_samples(n): prior_draws = [] while len(prior_draws) &lt; n: draw = sample_prior() if all([(np.linalg.norm(np.array(list(draw.values())) - np.array(list(prior_draw.values()))) &gt; 1) for prior_draw in prior_draws]): prior_draws.append(exp_params(draw)) return prior_draws def multi(prior_draws): current_best_loss = np.inf optimization_records = [] for i, θ_0 in enumerate(prior_draws): record = optimize_by_gradient_descent(θ_0) optimization_records.append(record) if record.loss.iloc[-1] &lt; current_best_loss: current_best_loss = i return current_best_loss, optimization_records . draws = prior_samples(10) best, optimization_records = multi(draws) . {&#39;k_on&#39;: 0.000355129128292772, &#39;k_off&#39;: 9.423609884821825, &#39;k_cat&#39;: 4.470995636194403, &#39;k_uncat&#39;: 2.0772038714946985} {&#39;k_on&#39;: 2.8480827623148253, &#39;k_off&#39;: 9.197642395681514, &#39;k_cat&#39;: 20.14328499435451, &#39;k_uncat&#39;: 0}}143451803811677} {&#39;k_on&#39;: 0.17170522469525323, &#39;k_off&#39;: 76547.89136205788, &#39;k_cat&#39;: 41.044138881241025, &#39;k_uncat&#39;: 0.004009095182967744} {&#39;k_on&#39;: 11.294961302609616, &#39;k_off&#39;: 76547.89052956448, &#39;k_cat&#39;: 43.61890328208817, &#39;k_uncat&#39;: 0}}} {&#39;k_on&#39;: 0.009367849153052348, &#39;k_off&#39;: 0.3955233519483537, &#39;k_cat&#39;: 744.717839809558, &#39;k_uncat&#39;: 1.7852579986150745e-05} {&#39;k_on&#39;: 0.009367849153052348, &#39;k_off&#39;: 0.3955233519483537, &#39;k_cat&#39;: 744.717839809558, &#39;k_uncat&#39;: 1.7852579986150745e-05}5} {&#39;k_on&#39;: 0.0007852087385352924, &#39;k_off&#39;: 10.58874288236055, &#39;k_cat&#39;: 2.540261829105342, &#39;k_uncat&#39;: 98.41012486638917} {&#39;k_on&#39;: 45.17064987437375, &#39;k_off&#39;: 0, &#39;k_cat&#39;: 52.98586213090315, &#39;k_uncat&#39;: 87.52931036488357}}}7.64913240524375} {&#39;k_on&#39;: 0.051060377668334996, &#39;k_off&#39;: 29.570465102111065, &#39;k_cat&#39;: 1.7781239681370957, &#39;k_uncat&#39;: 0.0038833634298043487} {&#39;k_on&#39;: 1.7090539900820565, &#39;k_off&#39;: 29.537918320711782, &#39;k_cat&#39;: 20.262526805303967, &#39;k_uncat&#39;: 0}} {&#39;k_on&#39;: 153.3316794415585, &#39;k_off&#39;: 0.022979198430110384, &#39;k_cat&#39;: 63.90177473382675, &#39;k_uncat&#39;: 2.333802534784028} {&#39;k_on&#39;: 153.3316794415585, &#39;k_off&#39;: 0.022979198430110384, &#39;k_cat&#39;: 63.90177473382675, &#39;k_uncat&#39;: 2.333802534784028}38} {&#39;k_on&#39;: 0.005482293707402161, &#39;k_off&#39;: 0.24263536864322946, &#39;k_cat&#39;: 2.4491959654589546, &#39;k_uncat&#39;: 1723.5044644873217} {&#39;k_on&#39;: 63.01783308293335, &#39;k_off&#39;: 0, &#39;k_cat&#39;: 65.74959885106847, &#39;k_uncat&#39;: 1722.3537896568193}}}3.504461862818} {&#39;k_on&#39;: 0.12232778946003305, &#39;k_off&#39;: 9.358979154201176, &#39;k_cat&#39;: 240.51882678767353, &#39;k_uncat&#39;: 0.05893830237039771} {&#39;k_on&#39;: 0.12233270638943898, &#39;k_off&#39;: 9.358979140598702, &#39;k_cat&#39;: 240.51882679192377, &#39;k_uncat&#39;: 0.058929594695499944} {&#39;k_on&#39;: 1.3198264601603056, &#39;k_off&#39;: 12.250716225523734, &#39;k_cat&#39;: 2.961522230479819, &#39;k_uncat&#39;: 0.6629218435280246} {&#39;k_on&#39;: 1.9583416178678057, &#39;k_off&#39;: 12.200992621105607, &#39;k_cat&#39;: 20.18827376357633, &#39;k_uncat&#39;: 0}}12658072336142956} {&#39;k_on&#39;: 0.04091883245276719, &#39;k_off&#39;: 15.961576684953053, &#39;k_cat&#39;: 1.9236975561145209, &#39;k_uncat&#39;: 2.1940986573907706} {&#39;k_on&#39;: 2.930659015437395, &#39;k_off&#39;: 15.781937122725841, &#39;k_cat&#39;: 20.162690387387592, &#39;k_uncat&#39;: 0}}922719848022795}} . default_kinetic_params . {&#39;k_on&#39;: 1, &#39;k_off&#39;: 500, &#39;k_cat&#39;: 30, &#39;k_uncat&#39;: 0.01, &#39;k_ms&#39;: 530.0, &#39;k_mp&#39;: 53000.0} . pd.DataFrame([optimization_record.loss for optimization_record in optimization_records]).T.add_prefix(&#39;prior draw &#39;).plot.line(logy=True, title=&#39;Loss trajectories for 10 draws from the prior&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Loss trajectories for 10 draws from the prior&#39;}&gt; . # define plot_optimization_trajectories def plot_optimization_trajectories(optimization_records): plt.rcParams[&#39;figure.figsize&#39;] = [12, 4*len(optimization_records)/2] fig, axs = plt.subplots(int(np.ceil(len(optimization_records)/2)), 2) for i, record in enumerate(optimization_records): P_traj = pd.concat([df[&#39;P&#39;] for df in record.u], axis=1) P_traj.columns = range(len(P_traj.columns)) plot_samples(P_traj.T, ax=axs[int(np.floor(i/2))][i%2]) axs[int(np.floor(i/2))][i%2].set_title(f&#39;prior draw {i}&#39;) . plot_optimization_trajectories(optimization_records) plt.tight_layout() . plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] . 3.2.2. Adjoint Method . How is Adjoint better? how does it scale differently with parameters? . $$ $$ # define euler_full(), which integrates the full kinetics with Euler&#39;s Method, and returns a trajectory def adjoint_euler_full(dt, steps, u=None, data=None, k_on=None, k_off=None, k_cat=None, k_uncat=None): λ = np.zeros(min(u.shape)) traj = [λ] for step in range(int(steps)): S, E, ES, P = u[-step] dgdu = np.zeros(min(u.shape)) if step in data: dgdu = 2*(data[step] - u[step]) # dfdu is a Jacobian dfdu = np.array([ [-k_on * E, -k_on * S, k_off, 0], [-k_on * E, -k_on * S - k_uncat * P, k_off + k_cat, -k_uncat], [-k_on * E, k_on * S + k_uncat * P, -k_off - k_cat, k_uncat], [0, k_uncat * P, k_cat, -k_uncat] ]) dλ = -λ @ dfdu - dgdu λ += dλ * dt traj.append(float(λ)) return traj[::-1] . # 1. solve forwards: euler_full # 2. solve adjoint DE # 3. evaluate integral . adjoint_euler_full(dt, steps, u=, data=default_traj_full.values, *kinetic_params) . def integrate(kinetic_params, dt=dt, initial_conditions=default_initial_conditions): [(_, E_0), (_, S_0)] = initial_conditions.items() t_eval = observations.index[1:] t_span = (0, t_eval[-1]) y0 = [S_0, E_0, 0, 0] kinetic_params = {name: 10**val for name, val in kinetic_params.items()} fun = lambda t,y: dy_full(t, y, E_0=E_0, S_0=S_0, **kinetic_params) try: sol = solve_ivp(fun, t_span, y0, t_eval=t_eval, first_step=dt, max_step=1e-2, method=&#39;LSODA&#39;) return sol.y[3] # Product except: return np.zeros(10) . 4. Conclusions . Michaelis menten isn&#39;t even right, that&#39;s why the databases are such a mess. You need the right scheme, and the right equations . 5. References . Gradient Descent for ODEs by Demetri Pananos . | A Database of Thermodynamic Quantities for the Reactions of Glycolysis and the Tricarboxylic Acid Cycle . | A database of thermodynamic properties of the reactions of glycolysis, the tricarboxylic acid cycle, and the pentose phosphate pathway . | . Thermodynamics of Glycolysis/III%3A_Reactivity_in_Organic_Biological_and_Inorganic_Chemistry_1/08%3A_Mechanisms_of_Glycolysis/8.08%3A_Thermodynamics_of_Glycolysis) | . BKMS-React database entry for glycolysis | . Bistability in Glycolysis Pathway as a Physiological Switch in Energy Metabolism | . Determination of the rate of hexokinase-glucose dissociation by the isotope-trapping method | . Quantitative Fundamentals of Molecular and Cellular Bioengineering | .",
            "url": "https://alexlenail.me/back_of_my_envelope/2021/05/24/Enzyme-Kinetic-Parameter-Inference.html",
            "relUrl": "/2021/05/24/Enzyme-Kinetic-Parameter-Inference.html",
            "date": " • May 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Orthogonal Functions",
            "content": "# imports import numpy as np import pandas as pd import matplotlib.pyplot as plt import scipy.stats %matplotlib inline plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] plt.rcParams[&#39;figure.dpi&#39;] = 140 π = np.pi exp = np.exp sin = np.sin cos = np.cos sqrt = np.sqrt . Fourier Basis . grid = 200 domain = [0, 2*π] dx = (domain[1]-domain[0])/grid grid = np.linspace(*domain, grid) def fourier(k, x): return sin(k*x)+cos(k*x) . n = 5 basis = pd.DataFrame({k: fourier(k, grid) for k in range(1,n)}, index=grid) ax = basis.plot.line(lw=0.4, xlim=domain) ax.axhline(0, c=&#39;black&#39;, lw=&#39;0.3&#39;) . &lt;matplotlib.lines.Line2D at 0x136a4e890&gt; . from scipy import integrate def compare_two(i, j): product = pd.Series(basis[i]*basis[j], name=&#39;product&#39;) product = pd.DataFrame([basis[i], basis[j], product]).T ax = product.plot.line(lw=0.5, color=[&#39;red&#39;, &#39;blue&#39;, &#39;purple&#39;]) ax.fill_between(grid, product[&#39;product&#39;], alpha=0.1) return integrate.trapz(product[&#39;product&#39;], x=product.index) . print(&#39;integral =&#39;, np.round(compare_two(3,4), 4)) . integral = -0.0 . &quot;fourier modes as eigenfunctions of the derivative operator&quot; What? . Polynomial Bases .",
            "url": "https://alexlenail.me/back_of_my_envelope/2020/12/04/orthogonal-functions.html",
            "relUrl": "/2020/12/04/orthogonal-functions.html",
            "date": " • Dec 4, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "",
          "url": "https://alexlenail.me/back_of_my_envelope/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Old Blog",
          "content": "",
          "url": "https://alexlenail.me/back_of_my_envelope/old/",
          "relUrl": "/old/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page7": {
          "title": "",
          "content": "{“/about/”:”http://alexlenail.me”,”/old/”:”https://alexlenail.medium.com/”} .",
          "url": "https://alexlenail.me/back_of_my_envelope/redirects.json",
          "relUrl": "/redirects.json",
          "date": ""
      }
      
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://alexlenail.me/back_of_my_envelope/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}