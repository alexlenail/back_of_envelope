{
  
    
        "post0": {
            "title": "Fourrier Basis",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import scipy.stats %matplotlib inline plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] plt.rcParams[&#39;figure.dpi&#39;] = 140 π = np.pi exp = np.exp sin = np.sin cos = np.cos sqrt = np.sqrt . %%html &lt;style&gt; body { font-family: -apple-system, BlinkMacSystemFont, &#39;Roboto&#39;, &quot;Segoe UI&quot;, Roboto, &quot;Helvetica Neue&quot;, Arial, sans-serif; } .rendered_html p { font-size: 1.5rem; } .rendered_html h1 { font-size: 3rem; } # div.text_cell_render.rendered_html { width: 60%; margin: auto; } &lt;/style&gt; . grid = 200 domain = [0, 2*π] dx = (domain[1]-domain[0])/grid grid = np.linspace(*domain, grid) def fourrier(k, x): return sin(k*x)+cos(k*x) . n = 5 basis = pd.DataFrame({k: fourrier(k, grid) for k in range(1,n)}, index=grid) ax = basis.plot.line(lw=0.4, xlim=domain) ax.axhline(0, c=&#39;black&#39;, lw=&#39;0.3&#39;) . &lt;matplotlib.lines.Line2D at 0x136a4e890&gt; . from scipy import integrate def compare_two(i, j): product = pd.Series(basis[i]*basis[j], name=&#39;product&#39;) product = pd.DataFrame([basis[i], basis[j], product]).T ax = product.plot.line(lw=0.5, color=[&#39;red&#39;, &#39;blue&#39;, &#39;purple&#39;]) ax.fill_between(grid, product[&#39;product&#39;], alpha=0.1) return integrate.trapz(product[&#39;product&#39;], x=product.index) . print(&#39;integral =&#39;, np.round(compare_two(3,4), 4)) . integral = -0.0 . &quot;fourrier modes as eigenfunctions of the derivative operator&quot; What? . Polynomial Bases .",
            "url": "https://alexlenail.me/back_of_my_envelope/2020/12/27/orthogonal-functions.html",
            "relUrl": "/2020/12/27/orthogonal-functions.html",
            "date": " • Dec 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "16.940 PS1",
            "content": "0. Setup . import numpy as np import pandas as pd import matplotlib.pyplot as plt import scipy.stats from sklearn.linear_model import LinearRegression %matplotlib inline plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] plt.rcParams[&#39;figure.dpi&#39;] = 140 exp = np.exp sin = np.sin cos = np.cos sqrt = np.sqrt . import matlab.engine eng = matlab.engine.start_matlab() . %%html &lt;style&gt; body { font-family: -apple-system, BlinkMacSystemFont, &#39;Roboto&#39;, &quot;Segoe UI&quot;, Roboto, &quot;Helvetica Neue&quot;, Arial, sans-serif; } .rendered_html p { font-size: 1.6rem; } .rendered_html h1 { font-size: 3rem; } # div.text_cell_render.rendered_html { width: 60%; margin: auto; } &lt;/style&gt; . 1 Warmup: life without a CLT . Write a simple Monte Carlo method to estimate the mean of X, i.e., $μ_X := E[X]$ . So this density is defined over the positive reals. So we need to be able to sample from the positive reals . Let&#39;s try inverse CDF method. First we need the CDF, then we need to invert it. To get the CDF, we integrate the pdf. . α = 1.5 def inverse_pareto_cdf(y): return (1-y)**(-1/α) . num_trials = 1000 num_steps = 500 . runs = pd.DataFrame(np.cumsum(inverse_pareto_cdf(np.random.random(size=(num_trials,num_steps))), axis=1)) / range(1,num_steps+1) runs.T.plot.line(legend=False, logy=True, alpha=0.01, color=&#39;blue&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x12527ca10&gt; . a) Describe the qualitative characteristics of the each sequence . Surprisingly high variance, even after 500 steps . runs.iloc[20:40].T.plot.line(legend=False, color=&#39;blue&#39;, lw=0.2) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x126063f50&gt; . b) Examine the sampling distribution of the estimator $x ̄_n$ at various values of $n$. In particular, use quantitative diagnostics to measure how this distribution departs from normality. . For example, what are the variance, skewness, kurtosis, etc. of this distribution as a function of $n$, and how do they depart from those predicted by the central limit theorem? . What about quantiles of the sampling distribution? . Does there seem to be an asymptotic distribution for any scaled version of this Pareto sum? . pd.DataFrame({&#39;var&#39;: runs.var(), &#39;skew&#39;: runs.skew(), &#39;kurt&#39;: runs.kurt()}).plot.line() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1262d2910&gt; . normal = pd.DataFrame(np.cumsum(np.random.normal(size=(num_trials,num_steps)), axis=1)) / range(1,num_steps+1) pd.DataFrame({&#39;var&#39;: normal.var(), &#39;skew&#39;: normal.skew(), &#39;kurt&#39;: normal.kurt()}).plot.line() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x12635fd90&gt; . 2 Rejection sampling versus importance sampling . def π(x): return exp(-(x**2/2)) * (sin(6*x)**2 + (3*cos(x)**2)*(sin(4*x)**2) + 1) . . (a) Using the $g$ specified above, implement a rejection sampling method that generates $n$ independent samples from $π$ and combines them in a Monte Carlo estimate, . def chunks(chunk_size, whole): return [chunk_size]*(whole//chunk_size) + [whole % chunk_size] def rejection_sampling(desired_n=None, fixed_t=None, batch_size=1000): n = 0 t = 0 I = 0 if desired_n: while n &lt; desired_n: # sample x from a normal x = np.random.normal(size=batch_size) # sample u from a uniform u = np.random.random(size=batch_size) x_ = x[π(x) &gt; u][:desired_n-n] n += len(x_) I += sum(x_**2) t += np.where(x == x_[-1])[0][0] elif fixed_t: for batch in chunks(batch_size, fixed_t): # sample x from a normal x = np.random.normal(size=batch) # sample u from a uniform u = np.random.random(size=batch) x_ = x[π(x) &gt; u] n += len(x_) I += sum(x_**2) t += batch return I/n, n, t . Keep track of how many samples t you must simulate from g in order to obtain n samples from π. This number t is the “stopping time” of the rejection sampler. . many_times = range(100) desired_n = 1000 pd.Series([rejection_sampling(desired_n=desired_n)[2]/desired_n for _ in many_times]).plot.kde(title=f&quot;Ratio of samples drawn from g to samples from π&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x12e9dae10&gt; . Conversely, you can fix $t$ and keep track of what random $n(t)$ you end up with. . Evaluate this estimator $I^{n(t)}_{AR}$ many times for a fixed t, and then empirically estimate the variance of this estimator, $Var[I^{n(t)}_{AR}]$ . fixed_t_grid = list(map(int, [1e3, 5e3, 1e4, 5e4])) rejection_sampling_runs = pd.DataFrame([rejection_sampling(fixed_t=fixed_t) for fixed_t in fixed_t_grid for _ in many_times], columns=[&#39;I&#39;,&#39;n&#39;,&#39;t&#39;]) rejection_sampling_runs . I n t . 0 0.599166 | 877 | 1000 | . 1 0.656422 | 870 | 1000 | . 2 0.575957 | 878 | 1000 | . 3 0.623628 | 881 | 1000 | . 4 0.652661 | 860 | 1000 | . ... ... | ... | ... | . 395 0.625553 | 43708 | 50000 | . 396 0.621092 | 43772 | 50000 | . 397 0.626054 | 43909 | 50000 | . 398 0.624040 | 43830 | 50000 | . 399 0.629682 | 43628 | 50000 | . 400 rows × 3 columns . rejection_sampling_variance = {fixed_t: rejection_sampling_runs[rejection_sampling_runs.t == fixed_t].I.var() for fixed_t in fixed_t_grid} rejection_sampling_variance . {1000: 0.0005003192482449542, 5000: 0.00013908207259769875, 10000: 7.469255003335081e-05, 50000: 1.1925059713683302e-05} . (b) Using the g specified above, implement a t-sample self-normalized importance sampling estimator of I, called IIS. Again, evaluate this estimator many times for a fixed t and empirically estimate its variance. . g = scipy.stats.norm().pdf def importance_sampling(fixed_t): # sample x from a normal x = np.random.normal(size=fixed_t) # grant each sample a weight w = π(x) / g(x) I = sum(x**2 * w) / sum(w) return I, fixed_t . importance_sampling_runs = pd.DataFrame([importance_sampling(fixed_t) for fixed_t in fixed_t_grid for _ in many_times], columns=[&#39;I&#39;,&#39;t&#39;]) importance_sampling_runs . I t . 0 0.807446 | 1000 | . 1 0.755656 | 1000 | . 2 0.928494 | 1000 | . 3 0.836112 | 1000 | . 4 0.859991 | 1000 | . ... ... | ... | . 395 0.815490 | 50000 | . 396 0.829864 | 50000 | . 397 0.840659 | 50000 | . 398 0.828794 | 50000 | . 399 0.830289 | 50000 | . 400 rows × 2 columns . importance_sampling_variance = {fixed_t: importance_sampling_runs[importance_sampling_runs.t == fixed_t].I.var() for fixed_t in fixed_t_grid} importance_sampling_variance . {1000: 0.001980423150376175, 5000: 0.0004299357739064024, 10000: 0.00024737834093724624, 50000: 4.333175581978758e-05} . (c) Now compare the variances of In(t) and It over a range of sample sizes t. Make a convergence plot. Comparing both estimators at the same t is in some sense the “fair” comparison, as both require simulating t samples from g. . pd.DataFrame([rejection_sampling_variance, importance_sampling_variance], index=[&#39;rejection_sampling&#39;, &#39;importance_sampling&#39;]).T.plot.line(logy=True, logx=True, title=&#39;Estimator variance as a function of number of samples&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x12ef59f10&gt; . (d) A third estimator of I can be produced by “recycling” the $t − n$ samples rejected by the accept/reject method. The marginal distribution of these rejected samples, up to a normalizing constant, is $Cg(x) − π ̃(x)$, where $C$ was the constant chosen so that $Cg(x) ≥ π ̃(x)$ for all $x ∈ R$. Since you know this marginal density, you can construct another self-normalized importance sampling estimator using the rejected samples; call it $I_{rej}$ . This can then be combined with $I_{AR}$ to yield a third estimator of I: . Evaluate the performance of this estimator over a range of t values and compare it with the others. . def recycling_sampling(t): # sample x from a normal x = np.random.normal(size=t) # sample u from a uniform u = np.random.random(size=t) x_acc = x[π(x) &gt; u] x_rej = x[π(x) &lt; u] n_acc = len(x_acc) n_rej = len(x_rej) w = g(x_rej) - π(x_rej) x_ = x_rej[np.argmin(g(x_rej) - π(x_rej))] C = π(x_) - g(x_) w = C*g(x_rej) - π(x_rej) I = (n_acc/t)*(1/n_acc)*sum(x_acc**2) + (n_rej/t)*sum(x_rej**2 * w) / sum(w) return I, t . recycling_sampling_runs = pd.DataFrame([recycling_sampling(fixed_t) for fixed_t in fixed_t_grid for _ in many_times], columns=[&#39;I&#39;,&#39;t&#39;]) recycling_sampling_runs . I t . 0 0.851971 | 1000 | . 1 0.878444 | 1000 | . 2 0.973715 | 1000 | . 3 0.897379 | 1000 | . 4 0.903163 | 1000 | . ... ... | ... | . 395 0.879955 | 50000 | . 396 0.872233 | 50000 | . 397 0.878072 | 50000 | . 398 0.885993 | 50000 | . 399 0.885343 | 50000 | . 400 rows × 2 columns . recycling_sampling_variance = {fixed_t: recycling_sampling_runs[recycling_sampling_runs.t == fixed_t].I.var() for fixed_t in fixed_t_grid} recycling_sampling_variance . {1000: 0.001193902523076437, 5000: 0.00019613303998738675, 10000: 8.534856190139974e-05, 50000: 2.1696300756501345e-05} . pd.DataFrame([rejection_sampling_variance, importance_sampling_variance, recycling_sampling_variance], index=[&#39;rejection_sampling&#39;, &#39;importance_sampling&#39;, &#39;recycling_sampling&#39;]).T.plot.line(logy=True, logx=True, title=&#39;Estimator variance as a function of number of samples&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x12f1f19d0&gt; . 3 Stochastic elliptic PDE . Consider a stochastic linear elliptic equation on a one-dimensional spatial domain: . $$ frac{ partial}{ partial x} left( k(x, omega) frac{ partial u(x, omega)}{ partial x} right) = -s(x) quad x in D = [0,1]$$ . with a deterministic source term $s(x)$, a deterministic Dirichlet boundary condition at $x = 1$, . $$ u(1, omega) = u_r $$ . and a random Neumann condition at $x = 0$ . $$k(x, omega) frac{ partial u}{ partial x} bigg rvert_{x=0} = −F( omega)$$ . The diffusivity $k(x, omega)$ and solution $u(x, omega)$ are stochastic processes defined on $D × Ω$, where $(Ω, U, P)$ is a probability space. $F(ω)$ is defined on the same probability space. This stochastic elliptic equation can model a host of physical phenomena, ranging from heat conduction in a heterogeneous material (where u is proportional to temperature) to fluid flow in a porous medium (where u could denote pressure and k is proportional to permeability). . 3.1 Problem parameters . μ_π = np.array([μ_F] + [μ_Y]*4) Σ_π = np.diag( [σ_F] + [σ_Y]*4) π = scipy.stats.multivariate_normal(mean=μ_π, cov=Σ_π) . resolution = 101 domain = np.linspace(0,1,resolution) xgrid = matlab.double(domain.tolist()) def diffusioneqn(F, Y1, Y2, Y3, Y4): &#39;&#39;&#39; F = flux at left-hand boundary, k*du/dx = -F Yk = log-diffusivity along 0.25*(k-1):0.25*k &#39;&#39;&#39; Y = [y.item() for y in [Y1, Y2, Y3, Y4]] F = F.item() # column vector of diffusivities at each point in the domain k = eng.transpose(matlab.double([exp(Y[int(x)]) for x in np.floor(domain / 0.2501)])) # source term, either a vector of values at points in xgrid or a constant source = 5.0 # rightbc = Dirichlet BC on right-hand boundary rightbc = 1.0 return np.asarray(eng.diffusioneqn(xgrid, F, k, source, rightbc)) . pd.DataFrame(diffusioneqn(*π.rvs(1)), index=domain).plot.line(title=&#39;solution for 1 draw of F and Y&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x13626de50&gt; . def diffusioneqn_multi(xs): return pd.DataFrame(np.concatenate([diffusioneqn(*x) for x in np.atleast_2d(xs)], axis=1), index=domain) def MC_diffusioneqn(num_samples=100): return diffusioneqn_multi(π.rvs(num_samples)) def query_point_stats(df): query_point = df.loc[0.60] mean = query_point.mean() var = query_point.var() return mean, var def plot_MC_diffusioneqn(df): fig, axs = plt.subplots(1, 2) df.plot.line(color=&#39;blue&#39;, lw=0.1, alpha=1, legend=False, ax=axs[0], title=f&#39;solution for {num_samples} runs&#39;) axs[0].axvline(0.6, lw=0.2, color=&#39;black&#39;) query_point = df.loc[0.60] query_point.plot.kde(ax=axs[1], title=&#39;distribution of values at 0.6&#39;) axs[1].axvline(df.loc[0.60].mean(), color=&#39;red&#39;, lw=0.5) . plot_MC_diffusioneqn(MC_diffusioneqn()) . 3.2 Control variates for Monte Carlo variance reduction . (a) . Use a Monte Carlo method to estimate $E[u(x = 0.6, ω)]$. Report your estimate along with its standard error (i.e., the estimated standard deviation of the estimator) and a 95% confidence interval, based a single n-sample run. Try this a few times (i.e., for independent “replicate” runs) and for different samples sizes n. . def query_point(num_samples_per_run=100, num_runs=20): plt.rcParams[&#39;figure.figsize&#39;] = [12, 10] plt.rcParams[&#39;figure.dpi&#39;] = 140 fig = plt.figure() gs = fig.add_gridspec(2, 2) ax1 = fig.add_subplot(gs[0, :]) ax2 = fig.add_subplot(gs[1, 0]) ax3 = fig.add_subplot(gs[1, 1]) df = pd.DataFrame([MC_diffusioneqn(num_samples_per_run).loc[0.60] for _ in range(num_runs)]).T ax = df.plot.kde(color=&#39;blue&#39;, lw=0.1, legend=False, xlim=[-20, 60], ax=ax1, title=f&#39;{num_runs} distributions of {num_samples_per_run} samples for values of the SPDE at 0.6&#39;) means = df.mean().values variances = df.var().values for μ in means: ax1.axvline(μ, color=&#39;red&#39;, lw=0.1) ci = [np.mean(means)-2*np.std(means), np.mean(means)+2*np.std(means)] pd.Series(means).plot.kde(ax=ax2, color=&#39;red&#39;, title=f&#39;Means: {np.round(np.mean(means), 2)} ± {np.round(2*np.std(means), 2)}&#39;) ax2.fill_between(*zip(*[(x, y) for x, y in zip(ax2.get_children()[0]._x, ax2.get_children()[0]._y) if ci[0] &lt; x &lt; ci[1]]) , alpha=0.1) ax2.axvline(np.mean(means), color=&#39;red&#39;) pd.Series(variances).plot.kde(ax=ax3, color=&#39;purple&#39;, title=&#39;Variances&#39;) ax3.axvline(np.mean(variances), color=&#39;red&#39;) stats = pd.DataFrame({ &#39;mean&#39;: { &#39;mean&#39;: np.mean(means), &#39;standard_error&#39;: np.std(means), }, &#39;variance&#39;: { &#39;mean&#39;: np.mean(variances), &#39;standard_error&#39;: np.std(variances), }}) return means, variances, stats . n100_means, n100_variances, n100_stats = query_point(num_samples_per_run=100, num_runs=20) . n200_means, n200_variances, n200_stats = query_point(num_samples_per_run=200, num_runs=20) . n25_means, n25_variances, n25_stats = query_point(num_samples_per_run=25, num_runs=20) . (b) . Now use a Monte Carlo method to estimate $Var[u(x = 0.6, ω)]$. Report your estimate along with its standard error. Again, try this a few times, and for different samples sizes n. . stats = pd.concat((n200_stats.T.stack(), n100_stats.T.stack(), n25_stats.T.stack()), axis=1) stats.columns = [&#39;n = 200&#39;, &#39;n = 100&#39;, &#39;n = 25&#39;] stats . n = 200 n = 100 n = 25 . mean mean 4.579548 | 4.391582 | 4.281548 | . standard_error 0.265125 | 0.338882 | 0.632220 | . variance mean 14.614834 | 11.713166 | 9.937806 | . standard_error 6.504405 | 5.757254 | 10.354863 | . (c) . Now we will re-attempt the mean estimate of part (a) with a control variate. While many control variates could be devised for the problem, let’s use a simple one: a linearization of the map $(f, y1, y2, y3, y4) mapsto u(x = 0.6)$. (You may construct the linearization however you wish. Note that the expectation of the linear map can be computed exactly.) . def build_linear_map(num_samples): X = π.rvs(num_samples) y = diffusioneqn_multi(X).loc[0.60].values return LinearRegression().fit(X, y) . linear_map = build_linear_map(10000) . expectation_of_linear_map = linear_map.predict([μ_π])[0] . Compare the variance of the Monte Carlo estimator with and without the control variate, for different numbers of samples n. What happens to the amount of variance reduction as you make μY much larger or much smaller? . num_samples = 1000 X = π.rvs(num_samples) approx = linear_map.predict(X) exact = diffusioneqn_multi(X).loc[0.60].values c = -np.cov(approx, exact)[0][1] / np.var(approx) . def control_variate_estimate(num_samples=100): X = π.rvs(num_samples) exact = diffusioneqn_multi(X).loc[0.60].values Z = exact + c*(linear_map.predict(X) - expectation_of_linear_map) return sum(exact) / num_samples, sum(Z) / num_samples . results = pd.DataFrame([control_variate_estimate() for _ in range(100)], columns=[&#39;exact&#39;, &#39;control variate&#39;]) results.var().rename(&#39;variance&#39;).to_frame() . variance . exact 0.262449 | . control variate 0.130040 | . 3.3 Importance sampling for rare events . Our second goal is to estimate the probability $p := P[u(x = 0.6, ω) &gt; u_0]$, with $u_0 = 40$. . (a) . Use a standard Monte Carlo method to estimate $p$. Estimate the standard deviation $ hat{σ}_n$ of your Monte Carlo estimate of this probability, for different sample sizes n. As an integrated performance metric, report the relative error per sample, defined as $ sqrt{n} hat{σ}_n/p$. . def h(x): return diffusioneqn_multi(x).loc[0.60] &gt; 40 def relative_error_per_sample(n, trials=20): ps = [sum(h(π.rvs(n))) / n for _ in range(trials)] p = np.mean(ps) σ̂ = np.std(ps) return n, p, σ̂, sqrt(n) * σ̂ / p . ns = [100, 200, 500] MC_estimates = pd.DataFrame([list(relative_error_per_sample(n)) for n in ns], columns=[&#39;n&#39;, &#39;p&#39;, &#39;σ̂&#39;, &#39;rel err&#39;]).set_index(&#39;n&#39;) MC_estimates . p σ̂ rel err . n . 100 0.00150 | 0.003571 | 23.804761 | . 200 0.00125 | 0.002165 | 24.494897 | . 500 0.00060 | 0.000917 | 34.156503 | . (b) . Now consider using importance sampling to improve the efficiency of your Monte Carlo procedure. Devise a biasing distribution and use it to estimate p. Compare the relative error per sample with that of the standard Monte Carlo method in part (a). . We particularly recommend using the cross-entropy method to construct a good biasing distribution. For simplicity, let your biasing distribution remain Gaussian, perhaps even with diagonal covariance. . def cross_entropy_independent_normals(num_samples_per_iteration=2000, num_iterations=10): g_i = π for _ in range(num_iterations): # sample from the current g x = g_i.rvs(num_samples_per_iteration) # evaluate h for each of those samples, and drop those samples which do not yield solutions &gt; 40 y = x[h(x)] #if len(y) == 0: sample again # take the sample mean and variance (MLE) of those samples which yield solutions &gt; 40. μ_ip1 = np.mean(y, axis=0) Σ_ip1 = np.diag(np.var(y, axis=0)) # Those are the parameters of the next distribution g_i = scipy.stats.multivariate_normal(mean=μ_ip1, cov=Σ_ip1) return g_i . g = cross_entropy_independent_normals() . def h_m_is(g, m=100): # sample from g x = g.rvs(m) # evaluate weights w = π.pdf(x) / g.pdf(x) # weighted sum return sum(w * h(x).astype(int)) / m . def relative_error_per_sample_IS(g, m, trials=30): ps = [h_m_is(g, m) for _ in range(trials)] p = np.mean(ps) σ̂ = np.std(ps) return m, p, σ̂, sqrt(m) * σ̂ / p . ms = [100, 200, 500] MC_is_estimates = pd.DataFrame([list(relative_error_per_sample_IS(g, m)) for m in ms], columns=[&#39;m&#39;, &#39;p&#39;, &#39;σ̂&#39;, &#39;rel err&#39;]).set_index(&#39;m&#39;) MC_is_estimates . p σ̂ rel err . m . 100 0.000011 | 0.000011 | 9.853601 | . 200 0.000012 | 0.000006 | 7.372491 | . 500 0.000016 | 0.000009 | 12.966332 | . Then experiment with more complex biasing distributions (e.g., correlated Gaussian distributions, multivariate t distributions) and see if they yield any further performance gains. . def cross_entropy_correlated_normals(num_samples_per_iteration=2000, num_iterations=10): g_i = π for _ in range(num_iterations): # sample from the current g x = g_i.rvs(num_samples_per_iteration) # evaluate h for each of those samples, and drop those samples which do not yeild solutions &gt; 40 y = x[h(x)] #if len(y) == 0: sample again # take the sample mean and variance (MLE) of those samples which yield solutions &gt; 40. μ_ip1 = np.mean(y, axis=0) if len(y) &gt; len(μ_ip1): Σ_ip1 = np.diag(np.var(y, axis=0)) else: Σ_ip1 = np.cov(y, rowvar=False) # Those are the parameters of the next distribution g_i = scipy.stats.multivariate_normal(mean=μ_ip1, cov=Σ_ip1) return g_i . g2 = cross_entropy_correlated_normals() . ms = [100, 200, 500] MC_is_estimates = pd.DataFrame([list(relative_error_per_sample_IS(g2, m)) for m in ms], columns=[&#39;m&#39;, &#39;p&#39;, &#39;σ̂&#39;, &#39;rel err&#39;]).set_index(&#39;m&#39;) MC_is_estimates . p σ̂ rel err . m . 100 0.001122 | 0.001316 | 11.726191 | . 200 0.000674 | 0.000500 | 10.504109 | . 500 0.000868 | 0.000332 | 8.557591 | .",
            "url": "https://alexlenail.me/back_of_my_envelope/2020/12/26/16.940-PS1.html",
            "relUrl": "/2020/12/26/16.940-PS1.html",
            "date": " • Dec 26, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fourrier Basis",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import scipy.stats %matplotlib inline plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] plt.rcParams[&#39;figure.dpi&#39;] = 140 π = np.pi exp = np.exp sin = np.sin cos = np.cos sqrt = np.sqrt . %%html &lt;style&gt; body { font-family: -apple-system, BlinkMacSystemFont, &#39;Roboto&#39;, &quot;Segoe UI&quot;, Roboto, &quot;Helvetica Neue&quot;, Arial, sans-serif; } .rendered_html p { font-size: 1.5rem; } .rendered_html h1 { font-size: 3rem; } # div.text_cell_render.rendered_html { width: 60%; margin: auto; } &lt;/style&gt; . grid = 200 domain = [0, 2*π] dx = (domain[1]-domain[0])/grid grid = np.linspace(*domain, grid) def fourrier(k, x): return sin(k*x)+cos(k*x) . n = 5 basis = pd.DataFrame({k: fourrier(k, grid) for k in range(1,n)}, index=grid) ax = basis.plot.line(lw=0.4, xlim=domain) ax.axhline(0, c=&#39;black&#39;, lw=&#39;0.3&#39;) . &lt;matplotlib.lines.Line2D at 0x136a4e890&gt; . from scipy import integrate def compare_two(i, j): product = pd.Series(basis[i]*basis[j], name=&#39;product&#39;) product = pd.DataFrame([basis[i], basis[j], product]).T ax = product.plot.line(lw=0.5, color=[&#39;red&#39;, &#39;blue&#39;, &#39;purple&#39;]) ax.fill_between(grid, product[&#39;product&#39;], alpha=0.1) return integrate.trapz(product[&#39;product&#39;], x=product.index) . print(&#39;integral =&#39;, np.round(compare_two(3,4), 4)) . integral = -0.0 . &quot;fourrier modes as eigenfunctions of the derivative operator&quot; What? . Polynomial Bases .",
            "url": "https://alexlenail.me/back_of_my_envelope/2020/10/11/orthogonal-functions.html",
            "relUrl": "/2020/10/11/orthogonal-functions.html",
            "date": " • Oct 11, 2020"
        }
        
    
  

  
  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page7": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://alexlenail.me/back_of_my_envelope/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}