{
  
    
        "post0": {
            "title": "Enzyme Kinetic Parameter Inference",
            "content": "# imports from itertools import combinations_with_replacement, product from collections import Counter from io import StringIO import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.ticker as mtick from matplotlib.colors import to_hex import scipy.stats import seaborn as sns from scipy.stats import multivariate_normal from scipy.interpolate import interp1d from scipy.stats.kde import gaussian_kde from sklearn.linear_model import LinearRegression from sklearn.gaussian_process.kernels import Matern import ipywidgets as widgets from IPython.display import display %config InlineBackend.figure_format = &#39;retina&#39; # from IPython.display import set_matplotlib_formats %matplotlib inline # set_matplotlib_formats(&#39;svg&#39;) plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] plt.rcParams[&#39;figure.dpi&#39;] = 140 plt.rcParams[&#39;agg.path.chunksize&#39;] = 10000 exp = np.exp sqrt = np.sqrt Π = np.prod π = np.pi N = np.random.normal def hex_to_rgb(h): return [int(h.lstrip(&#39;#&#39;)[i:i+2], 16)/256 for i in (0, 2, 4)] . # resize figure special function from IPython.core.display import Image import io def resize_fig(width, height): s = io.BytesIO() plt.savefig(s, format=&#39;png&#39;, bbox_inches=&quot;tight&quot;, dpi=200) plt.close() return Image(s.getvalue(), width=width, height=height) . 1. Background . $$ newcommand{ kon}{k_{ mathrm{on}}} newcommand{ koff}{k_{ mathrm{off}}} newcommand{ kcat}{k_{ mathrm{cat}}} newcommand{ kuncat}{k_{ mathrm{uncat}}} newcommand{ kms}{k_{m, mathrm{S}}} newcommand{ kmp}{k_{m, mathrm{P}}} newcommand{ dSdt}{ frac{d[ mathrm{S}]}{dt}} newcommand{ dEdt}{ frac{d[ mathrm{E}]}{dt}} newcommand{ dESdt}{ frac{d[ mathrm{ES}]}{dt}} newcommand{ dPdt}{ frac{d[ mathrm{P}]}{dt}}$$1.1 Enzyme Kinetics . Enzymes catalyze many critical chemical reactions in cells. . Describing a cell with a mathematical model (a long-standing goal of computational biologists) would entail modelling each enzyme-catalyzed chemical reaction. . However, although we may know the scheme for many enzymatic reactions (the responsible enzyme, the associated substrates, and resultant products) we are often missing many of the details needed to construct a faithful mathematical model of the reaction. . Let&#39;s begin by introducing the mathematical model used to describe enzymatic reaction schemes. Consider the following enzymatically-catalyzed (uni uni) chemical reaction scheme: . $$ E+S underset{ koff}{ overset{ kon}{ rightleftarrows}} ES underset{ kuncat}{ overset{ kcat}{ rightleftarrows}}E+P $$ . In this scheme E is an enzyme, S is its substrate, ES is the enzyme-substrate complex, which is an intermediate, and P is the product of the reaction. Each of those chemical species has a concentration in a fixed volume, which we denote with brackets (e.g. $[ mathrm{E}]$ = enzyme concentration). . If we make the simplifying assumption that the 4 molecular species are &#39;well-mixed&#39; in solution, we can invoke the &#39;Law of Mass Action&#39; under which the rate of each of the four included reactions is linear in the concentrations of the reactants (with an associated coefficient called the rate constant). The reactions in the above scheme are: enzyme-substrate association ($ kon$), dissociation ($ koff$), enzyme catalysis of substrate into product ($ kcat$), and enzyme-product re-association (&quot;uncatalysis&quot;, $ kuncat$). The &#39;direction&#39; of the reaction, and the designation of &#39;substrate&#39; and &#39;product&#39; is our choice -- the model is entirely symmetric, which is reflected in the associated ODEs: . $$ begin{aligned} frac{d[ mathrm{S}]}{dt} &amp;= k_{ mathrm{off}}[ mathrm{ES}] - k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] frac{d[ mathrm{E}]}{dt} &amp;= k_{ mathrm{off}}[ mathrm{ES}] - k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] + k_{ mathrm{cat}}[ mathrm{ES}] - k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] frac{d[ mathrm{ES}]}{dt} &amp;= - k_{ mathrm{off}}[ mathrm{ES}] + k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] - k_{ mathrm{cat}}[ mathrm{ES}] + k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] frac{d[ mathrm{P}]}{dt} &amp;= k_{ mathrm{cat}}[ mathrm{ES}] - k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] end{aligned}$$This differential equation model describing the (deterministic) chemical kinetics for an enzymatically-catalyzed reaction in well-mixed conditions contains 4 kinetic parameters, i.e. 4 degrees of freedom, which we do not know a priori. These will be the subject of inference. . Note: the intracellular environment is not best described as well-mixed, and models of &#8217;Macromolecular Crowding&#8217; have led to more accurate rate laws for these reactions in vivo. However, we will retain the well-mixed assumption for now. . 1.2 Parameter Inference . There are 3 typical problems associated with ODE models: . Supplied with a complete specification of the system, the forward problem is to integrate the differential equations from some initial conditions forwards in time and predict the trajectory of the system. This is what is typically meant by &quot;solving&quot; the ODE system, but exact analytical solutions are rare, and numerical methods are often brought to bear to approximate system trajectories. | Supplied with one or more trajectories (data) but incomplete specification of the system, the inverse problem is to estimate parameters of the system (coefficients in the ODE expressions). | Finally, given some manipulable inputs, the control problem is to drive the system towards some desired state. | . This post will explore a range of approaches for the inverse problem. Our goal will be to estimate the kinetic parameters of enzymatically-catalyzed chemical reactions from timeseries of concentrations of the molecular species. . Note: enzyme kinetic parameters are typically not inferred from metabolite timeseries data using the methods we will describe, but instead from specific enzyme assays. However, at the moment, these assays are limited to studying one enzyme at a time. The inference approaches described in this post can leverage data from emerging high-throughput assays. . The determination of the kinetic parameters for the enzymatic reactions of life is a major project, and reported values have been tabulated in databases such as BRENDA. However, my experience with these databases has been that the reported kinetic parameters appear to be severely unreliable. . 1.3 The Michaelis-Menten/Briggs-Haldane Approximation . Two assumptions commonly made at this point are: . to assume the initial substrate concentration is much larger than the enzyme concentration ($[ mathrm{S_0}] gg [ mathrm{E_0}]$). | to suppose that the rates of enzyme-substrate association ($ kon$) and dissociation ($ koff$) are greater than the rates of catalysis and uncatalysis (i.e. $ kon$, $ koff$ $ gg$ $ kcat$, $ kuncat$). | These assumptions permit a timescale separation argument called the &quot;Quasi-Steady-State Approximation&quot; (QSSA) permits us to set $ dESdt = 0$. From this approximation, we can derive the traditional Reversible Michaelis-Menten/Briggs-Haldane expression: . $$ begin{aligned} frac{d[ mathrm{P}]}{dt} &amp;= frac{ frac{ kcat , [ mathrm{E_T}] [ mathrm{S}]}{K_{m, mathrm{S}}} - frac{ koff , [ mathrm{E_T}] [ mathrm{P}]}{K_{m, mathrm{P}}}} {1+ frac{[ mathrm{S}]}{K_{m, mathrm{S}}} + frac{[ mathrm{P}]}{K_{m, mathrm{P}}}} frac{d[ mathrm{S}]}{dt} &amp;= - frac{d[ mathrm{P}]}{dt} end{aligned}$$in which we have introduced the &quot;Michaelis Constants&quot;: $K_{m, mathrm{S}} = frac{ koff + kcat}{ kon}$ and $K_{m, mathrm{P}} = frac{ koff + kcat}{ kuncat}$. . The QSSA reduces the system from 4 variables to 2. There are still 4 kinetic parameters to estimate in this reduced model. . Note: another assumption typically made at this point is to assume that catalysis is irreversible ($ kuncat = 0$), leading to a further simplified expression for the rate of product formation $ frac{d[ mathrm{P}]}{dt}$. However, this assumption is quite often inaccurate, so we will not make it. . 2. Exploring the Forward Model . 2.1 A Standard Example . Before we explore techniques to estimate enzyme kinetic parameters from timeseries data, we need to generate timeseries data to begin with. We can accomplish that by fixing kinetic parameters, then solving the forward problem. It will turn out that integrating the model forwards is a subroutine of both approaches to the inverse problem we&#39;ll see in this post. . In order to produce a trajectory, we need to set initial conditions. We&#39;ll integrate the reaction kinetics of a hypothetical in vitro experiment, in which a fixed quantity of enzyme and substrate are added to the reaction at the outset, then left to react. . Note: in vivo we would expect the concetration of enzyme to vary over time, and the substrate to be replenished. We will generalize this approach to a more biologically-relevant setting in a future post. . Our initial conditions are: . $[E]_0$, the initial enzyme concentration, is set to 1mM (miliMolar, i.e. 1000μM). | $[S]_0$, the initial substrate concentration is set to 10mM. | . default_initial_conditions = { &#39;E_0&#39;: 1e3, &#39;S_0&#39;: 10e3 } . Next, let&#39;s fix some generic rate constants: . $ kon ,$ of $10^6$ events per Mol per second, or 1 per μM per second, is a typical rate for enzyme-substrate binding. | $ koff ,$ of 500/s results in a $ koff$/$ kon$ = $k_d$ of 500 μM, which is a typical $k_d$. | $ kcat ,$ is 30/s, a fairly slow but respectable $ kcat$. | $ kuncat ,$ of $ frac{ kon}{10}$ is often considered as the boundary for the QSSA to hold (so 0.1 per μM per second). Let&#39;s use $ kuncat = frac{ kon}{100} = $ 0.01/μM for good measure. | . Our units are μM and seconds. . default_kinetic_params = { &#39;k_on&#39;: 1, &#39;k_off&#39;: 500, &#39;k_cat&#39;: 30, &#39;k_uncat&#39;: 0.01 } def k_ms(p): return (p[&#39;k_off&#39;] + p[&#39;k_cat&#39;]) / p[&#39;k_on&#39;] def k_mp(p): return (p[&#39;k_off&#39;] + p[&#39;k_cat&#39;]) / p[&#39;k_uncat&#39;] default_kinetic_params[&#39;k_ms&#39;] = k_ms(default_kinetic_params) default_kinetic_params[&#39;k_mp&#39;] = k_mp(default_kinetic_params) . There are a variety of numerical methods to integrate systems of differential equations. The most straightforward is Euler&#39;s method, which we&#39;ve written down explicitly for this system below: . # define euler_full(), which integrates the full kinetics with Euler&#39;s Method, and returns a trajectory def euler_full(dt, steps, E_0=None, S_0=None, k_on=None, k_off=None, k_cat=None, k_uncat=None, k_ms=None, k_mp=None): S = S_0 E = E_0 ES = 0 P = 0 traj = [[S, E, ES, P]] for _ in range(int(steps)): dS = k_off * ES - k_on * E * S dE = k_off * ES - k_on * E * S + k_cat * ES - k_uncat * E * P dES = k_on * E * S - k_off * ES - k_cat * ES + k_uncat * E * P dP = k_cat * ES - k_uncat * E * P S += dS * dt E += dE * dt ES += dES * dt P += dP * dt traj.append([S, E, ES, P]) return pd.DataFrame(traj, columns=[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;], index=np.around(np.linspace(0, dt*steps, int(steps)+1), 6)) . We&#39;ll also write down Euler&#39;s method for the Michaelis-Menten/Briggs-Haldane kinetics . # define euler_MM(), which integrates the Michaelis-Menten/Briggs-Haldane kinetics def euler_MM(dt, steps, E_0=None, S_0=None, k_on=None, k_off=None, k_cat=None, k_uncat=None, k_ms=None, k_mp=None): S = S_0 E = E_0 P = 0 traj = [P] for _ in range(int(steps)): dP = ((k_cat * E * S) / k_ms - (k_off * E * P) / k_mp) / (1 + S / k_ms + P / k_mp) dS = -dP P += dP * dt S += dS * dt traj.append(P) return pd.Series(traj, name=&#39;P_MM&#39;, index=np.around(np.linspace(0, dt*steps, int(steps)+1), 6)).to_frame() . To simulate the kinetics with little derivative steps, we need a step size, and a number of total steps: . dt = 1e-6 steps = 5e5 . Now we can integrate the reaction kinetics, and plot the trajectory. We&#39;ll overlay the Michaelis-Menten/Briggs-Haldane kinetics with dotted lines on top of the full kinetics (solid). . default_traj_full = euler_full(dt, steps, **default_kinetic_params, **default_initial_conditions) default_traj_mm = euler_MM(dt, steps, **default_kinetic_params, **default_initial_conditions) . # figure styles def fig_style(ax): for side in [&quot;right&quot;,&quot;top&quot;]: ax.spines[side].set_visible(False) ax.set_xlabel(&#39;time (s)&#39;, weight=&#39;bold&#39;) ax.set_ylabel(&#39;concentration (μM)&#39;, weight=&#39;bold&#39;) def param_string(E_0=None, S_0=None, k_on=None, k_off=None, k_cat=None, k_uncat=None, k_ms=None, k_mp=None): return f&#39;[k_on= {k_on}/μM/s] [k_off = {k_off}/s] [k_cat = {k_cat}/s] [k_uncat = {k_uncat}/μM/s] [E₀ = {int(E_0)}μM] [S₀ = {int(S_0)}μM]&#39; c = { &#39;S&#39;: &#39;dodgerblue&#39;, &#39;E&#39;: &#39;sienna&#39;, &#39;ES&#39;: &#39;blue&#39;, &#39;P&#39;: &#39;darkblue&#39;, &#39;S_MM&#39;: &#39;steelblue&#39;, &#39;P_MM&#39;: &#39;slateblue&#39;, &#39;k_on&#39;: &#39;mediumseagreen&#39;, &#39;k_off&#39;: &#39;olive&#39;, &#39;k_cat&#39;: &#39;darkgreen&#39;, &#39;k_uncat&#39;: &#39;darkgoldenrod&#39;, &#39;k_m&#39;: &#39;olivedrab&#39;, &#39;k_ms&#39;: &#39;forestgreen&#39;, &#39;k_mp&#39;: &#39;darkkhaki&#39;, } c = {k:to_hex(v) for k,v in c.items()} def color(columns): return [c[col] for col in columns] . ax = default_traj_full.plot.line(title=param_string(**default_initial_conditions, **default_kinetic_params), color=color(default_traj_full.columns)) default_traj_mm.plot.line(ax=ax, color=color(default_traj_mm.columns), linestyle=&#39;--&#39;) fig_style(ax) . We can plainly see the validity of the Quasi-Steady-State Approximation (QSSA) in action in the trajectory: Enzyme E and Substrate S rapidly form Enzyme-Substrate complex ES, the concentration of which remains relatively constant throughout the course of the reaction (recall the QSSA is the approximation that $ dESdt = 0$). Thus, the Michaelis-Menten/Briggs-Haldane product concentration trajectory P_MM well approximates the full kinetics trajectory for the concentration of product P, since the requisite assumptions are valid, namely, (1) $[ mathrm{S_0}] gg [ mathrm{E_0}]$ and (2) $ kon$, $ koff$ $ gg$ $ kcat$, $ kuncat$. . In practice, Michaelis-Menten/Briggs-Haldane kinetics are often assumed by default, risking the possibility of their misapplication. Let&#39;s take this opportunity to explore how the MM/BH kinetics diverge from the full kinetics when we violate the requisite assumptions. . 2.2: Breaking the Michaelis-Menten/Briggs-Haldane Assumptions: &nbsp; &nbsp; &nbsp; Initial Substrate:Enzyme Ratio . Suppose first the number of molecules of substrate is not much greater than the number of molecules of enzyme, which is a plausible regime for certain reactions in vivo. . initital_conditions = { &#39;E_0&#39;: 1e3, &#39;S_0&#39;: 2e3 } . traj_full = euler_full(dt, 2e5, **initital_conditions, **default_kinetic_params) traj_mm = euler_MM(dt, 2e5, **initital_conditions, **default_kinetic_params) ax = traj_full.plot.line(title=param_string(**initital_conditions, **default_kinetic_params), color=color(traj_full.columns)) traj_mm.plot.line(ax=ax, color=color(traj_mm.columns), linestyle=&#39;--&#39;) fig_style(ax) . Then P_MM worsens significantly as an estimate of P. . 2.3: Breaking the Michaelis-Menten/Briggs-Haldane Assumptions: &nbsp; &nbsp; &nbsp; Fast Enzyme-Substrate Complex Kinetics . Suppose further that the rates of association and dissociation of enzyme with subtstrate are not substantially faster than those of enzyme and product. . kinetic_params = { &#39;k_on&#39;: 0.05, &#39;k_off&#39;: 1, &#39;k_cat&#39;: 50, &#39;k_uncat&#39;: 0.5 } kinetic_params[&#39;k_ms&#39;] = k_ms(kinetic_params) kinetic_params[&#39;k_mp&#39;] = k_mp(kinetic_params) . traj_full = euler_full(dt, steps, **initital_conditions, **kinetic_params) traj_mm = euler_MM(dt, steps, **initital_conditions, **kinetic_params) ax = traj_full.plot.line(title=param_string(**initital_conditions, **kinetic_params), color=color(traj_full.columns)) traj_mm.plot.line(ax=ax, color=color(traj_mm.columns), linestyle=&#39;--&#39;) fig_style(ax) . Then the Michaelis-Menten/Briggs-Haldane kinetics diverge further. . In each of these latter trajectories, the criteria to make the Michaelis-Menten/Briggs-Haldane approximation are violated, leading to poor approximations to the full kinetics. We belabor this point here because in the following, we will seek to infer the parameters of the kinetics, and our inference will fit poorly if we fit to inappropriate kinetic expressions. . 2.4: Comparing Integrators . All of the above trajectories are generated by Euler&#39;s Method, the most intuitive ODE integration technique. Unfortunately, Euler&#39;s Method&#39;s naïvete has drawbacks: . The size of the error is large | It&#39;s slow, due to the uniform size of the timesteps. | . A popular alternative which reconciles these drawbacks is the 4th order Runge Kutta Method (abbreviated RK4) which is the default integration method of scipy&#39;s integrate package. . Due to it&#39;s superior speed and accuracy, we&#39;ll use this method during inference. As a sanity check, we compare our Euler Method code to scipy&#39;s RK4: . # define scipy_full and scipy_MM functions (and helpers) to integrate chemical kinetics with RK4 from scipy.integrate import solve_ivp def dy_full(t, y, E_0=None, S_0=None, k_on=None, k_off=None, k_cat=None, k_uncat=None, k_ms=None, k_mp=None): # Y ordered S,E,ES,P dy = [0]*4 dy[0] = k_off * y[2] - k_on * y[1] * y[0] dy[1] = k_off * y[2] - k_on * y[1] * y[0] + k_cat * y[2] - k_uncat * y[1] * y[3] dy[2] = k_on * y[1] * y[0] - k_off * y[2] - k_cat * y[2] + k_uncat * y[1] * y[3] dy[3] = k_cat * y[2] - k_uncat * y[1] * y[3] return dy def dy_MM(t, y, E_0=None, S_0=None, k_on=None, k_off=None, k_cat=None, k_uncat=None, k_ms=None, k_mp=None): # Y ordered S,P dy = [0]*2 dy[1] = ((k_cat * E_0 * y[0]) / k_ms - (k_off * E_0 * y[1]) / k_mp) / (1 + y[0] / k_ms + y[1] / k_mp) dy[0] = -dy[1] return dy def scipy_full(dt, steps, kinetic_params, E_0=None, S_0=None): t = np.linspace(0,dt*steps,1001) t_span = (t[0], t[-1]) y0 = [S_0, E_0, 0, 0] sol = solve_ivp(lambda t,y: dy_full(t,y, E_0=E_0, S_0=S_0, **kinetic_params), t_span, y0, first_step=dt) return pd.DataFrame(sol.y.T, index=sol.t, columns=[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;]) def scipy_MM(dt, steps, kinetic_params, E_0=None, S_0=None): t = np.linspace(0,dt*steps,1001) t_span = (t[0], t[-1]) y0 = [S_0, 0] sol = solve_ivp(lambda t,y: dy_MM(t,y, E_0=E_0, S_0=S_0, **kinetic_params), t_span, y0, first_step=dt) return pd.DataFrame(sol.y.T, index=sol.t, columns=[&#39;S&#39;, &#39;P&#39;])[&#39;P&#39;].rename(&#39;P_MM&#39;).to_frame() . # solve the system with both integrators, for default parameters scipy_full_traj = scipy_full(dt, steps, default_kinetic_params, **default_initial_conditions) euler_full_traj = euler_full(dt, steps, **default_kinetic_params, **default_initial_conditions) ax = scipy_full_traj.plot.line(title=param_string(**default_initial_conditions, **default_kinetic_params), color=color(scipy_full_traj.columns), alpha=0.5) euler_full_traj.plot.line(ax=ax, color=color(euler_full_traj.columns), linestyle=&#39;--&#39;) fig_style(ax) . # solve the system with both integrators, for unusual parameters kinetic_params = { &#39;k_on&#39;: 0.02, &#39;k_off&#39;: 5, &#39;k_cat&#39;: 10, &#39;k_uncat&#39;: 0.00001, } kinetic_params[&#39;k_ms&#39;] = k_ms(kinetic_params) kinetic_params[&#39;k_mp&#39;] = k_mp(kinetic_params) import time start = time.process_time() scipy_full_traj = scipy_full(dt, steps, kinetic_params, **initital_conditions) scipy_time = time.process_time() - start start = time.process_time() euler_full_traj = euler_full(dt, steps, **kinetic_params, **initital_conditions) euler_time = time.process_time() - start ax = scipy_full_traj.plot.line(title=param_string( **initital_conditions,**kinetic_params), color=color(scipy_full_traj.columns), alpha=0.5) euler_full_traj.plot.line(ax=ax, color=color(euler_full_traj.columns), linestyle=&#39;--&#39;) fig_style(ax) . The lack of deviation gives us confidence both integration techniques are accurate. Meanwhile, . f&#39;our naïve code takes {round(euler_time, 2)}s, whereas the optimized scipy code takes {round(scipy_time, 4)}s to generate the same trajectory.&#39; . &#39;our naïve code takes 1.27s, whereas the optimized scipy code takes 0.0082s to generate the same trajectory.&#39; . 3. Inference . We have seen how the trajectory of the chemical system is a function of the kinetic parameters. We would now like to invert that function to recover the kinetic parameters from an observed trajectory. . Suppose we know the initial concentrations of Enzyme E and Substrate S, and we measure the concentration of product P over the course of the reaction, which yields the following dataset: . # plot inverse problem setting observations = default_traj_full.loc[np.arange(10+1)/20, &#39;P&#39;] ax = observations.plot.line(marker=&#39;o&#39;, lw=0, color=color([&#39;P&#39;]), legend=True) default_traj_full.loc[0, [&#39;E&#39;, &#39;S&#39;]].to_frame().T.plot.line(ax=ax, marker=&#39;o&#39;, lw=0, color=color(default_traj_full.columns), legend=True) fig_style(ax) . We will use noiseless measurements for inference. Unlike some of our other assumptions, this one won&#39;t make a significant difference. . Note: If we had measured $ dPdt$ for various (linearly independent) concentrations of $[ mathrm{S}]$, $[ mathrm{P}]$, and $[ mathrm{E}]_0$ (as in an in vitro enzyme assay) we could use a nonlinear regression with the Michaelis-Menten/Briggs-Haldane expression for $ dPdt$. Concretely, supposing we had a set of measurements for the variables in blue, a nonlinear regression would permit us to fit the constants in red: $$ color{blue}{ dPdt} = frac{ frac{ color{red}{ kcat} , color{blue}{[ mathrm{E_T}]} color{blue}{[ mathrm{S}]}} { color{red}{K_{m, mathrm{S}}}} - frac{ color{red}{ koff} , color{blue}{[ mathrm{E_T}]} color{blue}{[ mathrm{P}]}}{ color{red}{K_{m, mathrm{P}}}}} {1+ frac{ color{blue}{[ mathrm{S}]}}{ color{red}{K_{m, mathrm{S}}}} + frac{ color{blue}{[ mathrm{P}]}}{ color{red}{K_{m, mathrm{P}}}}} $$ If we had assumed the reaction were irreversible, the Michaelis-Menten/Briggs-Haldane expression would have simplified to $$ color{blue}{ dPdt} = frac{ color{red}{ kcat} , color{blue}{[ mathrm{E_T}]} color{blue}{[ mathrm{S}]}} { color{red}{K_{m, mathrm{S}}} + color{blue}{[ mathrm{S}]}} $$ Where $ color{red}{ kcat} , color{blue}{[ mathrm{E_T}]}$ is often consolidated as $ color{red}{V_{max}}$. To recap, we take a different approach because: Simultaneous measurements of the activity many enzymes in cells might inform us about $[ mathrm{S}]$, $[ mathrm{P}]$, and perhaps $[ mathrm{E}]$ but not $ dPdt$. We would also presumably not be able to approximate $ dPdt$ via finite differences, due to the relative sparsity of the measurement compared to the rates of the reactions. | This approach would produce spurious estimates of the kinetic parameters in cases in which the Quasi-Steady-State Approximation is invalid (see §2.2, §2.3) which may often be the case in vivo. | At the moment, I believe there are no methods for the inverse problem which are not variants of the two methods I will describe, and importantly, no methods which do not iterate a loop, solving the forward problem at each iteration. There are two types of approaches to solving this inverse problem. We will explore the simplest variant of each type. . 3.1 Bayesian Approach: Inference by Sampling . [We assume the reader is familiar with Bayesian Inference in other settings.] . The goal of the Bayesian approach is to determine a posterior over the 4D space spanned by the kinetic parameters. The posterior is the product of the prior and likelihood (up to a constant factor). Thus the Bayesian Inference approach entails defining a prior and a likelihood. . 3.1.1. Prior . If the kinetic parameters of the enzyme under study are not unlike the kinetic parameters of enzymes studied in the past, then the empirical distribution of kinetic parameters of enzymes studied in the past is a good prior for the parameters of this enzyme. . Since databases of observed enzyme kinetic parameters (e.g. BRENDA, SabioRK) are difficult to work with, we&#39;ll use a previously curated set of kinetic parameters from the supplement of The Moderately Efficient Enzyme: Evolutionary and Physicochemical Trends Shaping Enzyme Parameters. . If we knew what sort of enzyme we were studying (which EC class) we could narrow our prior to just those kinetic parameters observed for enzymes of that class. . This database lists $k_{ mathrm{m}}$ and $ kcat$ for both &quot;forwards&quot; and &quot;reverse&quot; reactions with respect to which direction biologists believe is &quot;productive&quot;, from which we can parlay distributions for $ kms$ and $ kcat$ from reactions in the forwards direction, and $ kmp$ and $ koff$ from reverse reactions. . # import kinetic parameter database df = pd.read_excel(&#39;../data/Enzyme_Kinetic_Parameter_Inference/Moderately_Efficient_Enzyme/bi2002289_si_003.xls&#39;, 1)[[&#39;Reaction direction (KEGG)&#39;,&#39;KM (µM)&#39;,&#39;kcat (1/sec)&#39;]] empirical_kms = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == 1, &#39;KM (µM)&#39;].dropna().rename(&#39;k_ms&#39;) empirical_kmp = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == -1, &#39;KM (µM)&#39;].dropna().rename(&#39;k_mp&#39;) empirical_kcat = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == 1, &#39;kcat (1/sec)&#39;].dropna().rename(&#39;k_cat&#39;) empirical_koff = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == -1, &#39;kcat (1/sec)&#39;].dropna().rename(&#39;k_off&#39;) empirical_joint_forward_params = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == 1, [&#39;KM (µM)&#39;,&#39;kcat (1/sec)&#39;]].dropna().rename(columns={&#39;KM (µM)&#39;:&#39;k_ms&#39;, &#39;kcat (1/sec)&#39;:&#39;k_cat&#39;}) empirical_joint_reverse_params = df.loc[df[&#39;Reaction direction (KEGG)&#39;] == -1, [&#39;KM (µM)&#39;,&#39;kcat (1/sec)&#39;]].dropna().rename(columns={&#39;KM (µM)&#39;:&#39;k_mp&#39;, &#39;kcat (1/sec)&#39;:&#39;k_off&#39;}) . # figure styles def fig_style_2(ax): for side in [&quot;right&quot;,&quot;top&quot;,&quot;left&quot;]: ax.spines[side].set_visible(False) ax.get_yaxis().set_visible(False) . # plot km distribution in log-space log_empirical_kms = np.log(empirical_kms) log_empirical_kmp = np.log(empirical_kmp) log_kms_normal = scipy.stats.norm(loc=log_empirical_kms.mean(), scale=log_empirical_kms.std()) log_kmp_normal = scipy.stats.norm(loc=log_empirical_kmp.mean(), scale=log_empirical_kmp.std()) ax = log_empirical_kms.plot.hist(bins=500, alpha=0.3, density=1, legend=True) log_empirical_kmp.plot.hist(bins=500, ax=ax, alpha=0.3, density=1, legend=True) ax.set_xlabel(&#39;log₁₀(k_m[µM]) histogram&#39;, weight=&#39;bold&#39;) fig_style_2(ax) # x1 = np.linspace(log_kms_normal.ppf(0.01), log_kms_normal.ppf(0.99), 100) # ax.plot(x1, log_kms_normal.pdf(x1)*ax.get_ylim()[1]*3, &#39;-&#39;, lw=0.6, color=&#39;dodgerblue&#39;) # x2 = np.linspace(log_kmp_normal.ppf(0.01), log_kmp_normal.ppf(0.99), 100) # ax.plot(x2, log_kmp_normal.pdf(x2)*ax.get_ylim()[1]*3, &#39;-&#39;, lw=0.8, color=&#39;peru&#39;) . This plot is surprising: according to this database, enzymes appear to have roughly equal binding affinity for their substrates and products. . # plot kcat distribution in log-space log_empirical_kcat = np.log(empirical_kcat) log_empirical_koff = np.log(empirical_koff) log_kcat_normal = scipy.stats.norm(loc=log_empirical_kcat.mean(), scale=log_empirical_kcat.std()) log_koff_normal = scipy.stats.norm(loc=log_empirical_koff.mean(), scale=log_empirical_koff.std()) ax = log_empirical_kcat.plot.hist(bins=500, alpha=0.3, density=1, legend=True) log_empirical_koff.plot.hist(bins=500, ax=ax, alpha=0.3, density=1, legend=True) x1 = np.linspace(log_kcat_normal.ppf(0.01), log_kcat_normal.ppf(0.99), 100) ax.plot(x1, log_kcat_normal.pdf(x1)*ax.get_ylim()[1]*3, &#39;-&#39;, lw=0.4, color=&#39;dodgerblue&#39;) x2 = np.linspace(log_koff_normal.ppf(0.01), log_koff_normal.ppf(0.99), 100) ax.plot(x2, log_koff_normal.pdf(x2)*ax.get_ylim()[1]*3, &#39;-&#39;, lw=0.6, color=&#39;peru&#39;) ax.set_xlabel(&#39;log₁₀(k_cat[1/s]) histogram&#39;, weight=&#39;bold&#39;) fig_style_2(ax) . On the other hand, they have a fairly strong preference for catalyzing the reaction biologists think of as forwards (~10x). . Since these empirical distributions over $ kms$ and $ kcat$ in the forwards direction and $ kmp$ and $ koff$ in the reverse direction look sufficiently like normals in log space, so we&#39;ll treat them as lognormals. However, we would like our inference procedure to estimate the semantic parameters $ kon$, $ koff$, $ kcat$, and $ kuncat$. We can rearrange the expressions for $ kms$ and $ kmp$ to get expressions for the two parameters we&#39;re missing: . $$ kon = frac{ koff + kcat}{ kms} quad mathrm{and} quad kuncat = frac{ koff + kcat}{ kmp}$$ . Conveniently, the ratio of lognormal variables $ frac{X_1}{X_2}$ is also lognormal with $ mu_{1/2} = mu_1 - mu_2$ and $ sigma^2_{1/2} = sigma^2_1 + sigma^2_2 - sigma_{x_1, x_2}$. In order to use that fact, we say the sum of the random variables $ koff + kcat$ is also log-normally distributed. We compute its mean and variance empirically. . kcat_plus_koff = pd.Series(np.repeat(empirical_kcat.values, len(empirical_koff)) + np.tile(empirical_koff.values, len(empirical_kcat))) log_kcat_plus_koff_mean = np.log10(kcat_plus_koff).mean() log_kcat_plus_koff_var = np.log10(kcat_plus_koff).var() . This permits us to produce empirical distributions for $ kon$ and $ kuncat$, . log_kon_normal = scipy.stats.norm(loc=log_kcat_plus_koff_mean-log_empirical_kms.mean(), scale=sqrt(log_kcat_plus_koff_var+log_empirical_kms.var())) log_kuncat_normal = scipy.stats.norm(loc=log_kcat_plus_koff_mean-log_empirical_kmp.mean(), scale=sqrt(log_kcat_plus_koff_var+log_empirical_kmp.var())) . which, along with our empirical distributions for $ koff$ and $ kcat$, define a prior over the 4 kinetic parameters we wish to infer. . def prior_pdf(k_on=None, k_off=None, k_cat=None, k_uncat=None): return ( log_kon_normal.pdf(np.log10(k_on)) * log_koff_normal.pdf(np.log10(k_off)) * log_kcat_normal.pdf(np.log10(k_cat)) * log_kuncat_normal.pdf(np.log10(k_uncat))) def prior_logpdf(k_on=None, k_off=None, k_cat=None, k_uncat=None): return ( log_kon_normal.logpdf(np.log10(k_on)) + log_koff_normal.logpdf(np.log10(k_off)) + log_kcat_normal.logpdf(np.log10(k_cat)) + log_kuncat_normal.logpdf(np.log10(k_uncat))) def sample_log_prior(): # returns [k_on, k_off, k_cat, k_uncat] return { &#39;k_on&#39;: log_kon_normal.rvs(), &#39;k_off&#39;: log_koff_normal.rvs(), &#39;k_cat&#39;: log_kcat_normal.rvs(), &#39;k_uncat&#39;: log_kuncat_normal.rvs()} . Now that we have a prior, let&#39;s examine where the default parameters introduced in §2.1 land in this distribution. We had claimed they were &quot;typical&quot;. . # plot log-space kinetic parameter distributions, with default parameters presented previously overlaid fig, axs = plt.subplots(2,2,constrained_layout=True) def plot_distrib(distrib, ax, title, param): ax.set_xlim(-10,10) ax.set_ylim(0,0.15) x = np.linspace(distrib.ppf(0.001), distrib.ppf(0.999), 100) y = distrib.pdf(x) color = c[param] ax.plot(x, y, &#39;-&#39;, lw=0.7, color=color) ax.fill_between(x, 0, y, color=color, alpha=0.1) ax.axvline(np.log10(default_kinetic_params[param]), 0, 1, linestyle=&#39;--&#39;, color=color) ax.xaxis.set_ticks(np.arange(-10, 10.1, 2)) ax.set_xlabel(title, weight=&#39;bold&#39;) fig_style_2(ax) plot_distrib(log_kon_normal, axs[0][0], &#39;log₁₀(k_on[µM])&#39;, &#39;k_on&#39;) plot_distrib(log_koff_normal, axs[0][1], &#39;log₁₀(k_off[1/s])&#39;, &#39;k_off&#39;) plot_distrib(log_kuncat_normal, axs[1][0], &#39;log₁₀(k_uncat[µM])&#39;, &#39;k_uncat&#39;) plot_distrib(log_kcat_normal, axs[1][1], &#39;log₁₀(k_cat[1/s])&#39;, &#39;k_cat&#39;) . We might ask whether these are correlated lognormals . pp = sns.pairplot(np.log10(empirical_joint_reverse_params), kind=&quot;kde&quot;, plot_kws={&#39;linewidths&#39;:0.5, &#39;color&#39;:&#39;grey&#39;}) k_mp_univariate_density = pp.diag_axes[0].get_children()[0] k_mp_univariate_density.set_edgecolor(c[&#39;k_mp&#39;]) k_mp_univariate_density.set_facecolor(hex_to_rgb(c[&#39;k_mp&#39;]) + [0.1]) k_off_univariate_density = pp.diag_axes[1].get_children()[0] k_off_univariate_density.set_edgecolor(c[&#39;k_off&#39;]) k_off_univariate_density.set_facecolor(hex_to_rgb(c[&#39;k_off&#39;]) + [0.1]) resize_fig(400, 400) . pp = sns.pairplot(np.log10(empirical_joint_forward_params), kind=&quot;kde&quot;, plot_kws={&#39;linewidths&#39;:0.5, &#39;color&#39;:&#39;darkolivegreen&#39;}) k_ms_univariate_density = pp.diag_axes[0].get_children()[0] k_ms_univariate_density.set_edgecolor(c[&#39;k_ms&#39;]) k_ms_univariate_density.set_facecolor(hex_to_rgb(c[&#39;k_ms&#39;]) + [0.1]) k_cat_univariate_density = pp.diag_axes[1].get_children()[0] k_cat_univariate_density.set_edgecolor(c[&#39;k_cat&#39;]) k_cat_univariate_density.set_facecolor(hex_to_rgb(c[&#39;k_cat&#39;]) + [0.1]) resize_fig(400, 400) . Not enough to include covariances in the prior. We set the prior covariance to be a diagonal matrix: . prior_cov = np.diag([log_kon_normal.var(), log_koff_normal.var(), log_kcat_normal.var(), log_kuncat_normal.var()]) . 3.1.2. Likelihood . We need to define a likelihood $p(D| theta)$ which measures the probability of producing the observed data given settings of the kinetic parameters $ theta = [ kon, koff, kcat, kuncat]^ intercal$. Our data $D = {[ mathrm{P}]_t : t in 0...0.5 }$ are an observed trajectory of concentrations of reaction product P. The kinetic parameters predict a trajectory of concentrations of P. Therefore our likelihood should measure the distance between the observed and predicted trajectories $ {[ mathrm{P}]_t }$. . How far should the predicted trajectory be allowed to stray from the measured $ {[ mathrm{P}]_t }$? The likelihood is really our statement about the presumed noise in our measurements. If we believe our measurements are noiseless, then our likelihood should concentrate tightly around our measurements (a dirac $ delta$ in the limit), and we would only admit kinetic parameters that interpolate the observed $ {[ mathrm{P}]_t }$ exactly. However, in reality, no measurement is noiseless, so we propose the following noise model: . Supposing the detection of each molecule of P is independent with error rate $ sigma$ then the estimate for $[ mathrm{P}]$ is gaussian-distributed $ sim mathcal{N}([P]_t, sigma sqrt{[P]_t})$. The independence of the observations causes the variance of the gaussian to grow only as the square root of the concentration, via a Central Limit Theorem argument. We can represent this noise model/likelihood visually as: . σ = 3 # arbitrary magic number . # plot intuition for likelihood definition plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] ax = observations.plot.line(marker=&#39;.&#39;, lw=0, color=color([&#39;P&#39;]), legend=True, markersize=2) range = np.linspace(0,10000,1000) for x, y in observations.iloc[1:].items(): distrib = scipy.stats.norm(loc=y, scale=np.sqrt(y)*σ) ax.fill_betweenx(range, [x]*1000, x2=x+distrib.pdf(range)/distrib.pdf(y)/150, color=color([&#39;P&#39;]), alpha=0.2) fig_style(ax) . The likelihood of certain kinetic parameters is thus the product of the probabilites of . , then, is $ displaystyle prod_{t=0}^{0.5} p_t([ mathrm{P}]_t)$ where $p_t sim mathcal{N}([P]_t, sigma sqrt{[P]_t})$ . Which leaves us with a &quot;hyperparameter&quot; $ sigma$. . likelihood_dist = multivariate_normal(mean=observations.values[1:], cov=σ * np.diag(sqrt(observations.values[1:]))) def likelihood_logpdf(ut): return likelihood_dist.logpdf(ut) . def integrate(kinetic_params, dt=dt, initial_conditions=default_initial_conditions): [(_, E_0), (_, S_0)] = initial_conditions.items() t_eval = observations.index[1:] t_span = (0, t_eval[-1]) y0 = [S_0, E_0, 0, 0] sol = solve_ivp(lambda t,y: dy_full(t,y, E_0=E_0, S_0=S_0, **kinetic_params), t_span, y0, t_eval=t_eval, first_step=dt) return sol.y[3] . 3.1.3. Metropolis-Hastings . We can now evaluate the prior $p( theta)$ and the likelihood $p(D| theta)$ of kinetic parameters $ theta = [ kon, koff, kcat, kuncat]^ intercal$. Those two distributions permit us to elaborate an Markov Chain Monte Carlo (MCMC) routine to sample from the posterior $p( theta|D) propto p(D| theta) cdot p( theta)$. The algorithm is as follows: . Repeat: . Draw kinetic parameters from the proposal distribution. | Integrate the system with the proposed kinetic parameters. | Evaluate the likelihood of the trajectory generated in step 2. | Accept/Reject the proposal by a Metropolis-Hastings criterion. | Append the current kinetic parameters to the Markov Chain. | Construct a proposal distribution around the current kinetic parameters. | def MH_MCMC(chain_length=1e3): θt = sample_prior() ut = integrate(θt) πt = -likelihood_logpdf(ut) - prior_logpdf(**θt) cov = prior_cov i = 0 accept_ratio = 0 chain = [] samples = [] while i &lt; chain_length: θtp1 = proposal(θt, cov) utp1 = integrate(θtp1) πtp1 = -likelihood_logpdf(utp1) - prior_logpdf(**θtp1) if πtp1 - πt &lt; -np.log(np.random.rand()): θt, ut, πt = θtp1, utp1, πtp1 accept_ratio += 1 chain.append(θt) samples.append(ut.values) i += 1 if i % 1e2 == 0 and i &gt; 0: cov = np.cov(np.array(chain), rowvar=False) print(i, end=&#39; r&#39;) return pd.DataFrame(chain, columns=[&#39;kon&#39;, &#39;koff&#39;, &#39;kcat&#39;, &#39;kuncat&#39;]), pd.DataFrame(samples), accept_ratio/chain_length, cov . But we need to define a proposal density . def proposal(θt, cov): μ = [np.log10(θt[&#39;k_on&#39;]), np.log10(θt[&#39;k_off&#39;]), np.log10(θt[&#39;k_cat&#39;]), np.log10(θt[&#39;k_uncat&#39;])] log_θtp1 = np.random.multivariate_normal(μ, cov) θtp1 = {&#39;k_on&#39;: exp(log_θtp1[0]), &#39;k_off&#39;: exp(log_θtp1[1]), &#39;k_cat&#39;: exp(log_θtp1[2]), &#39;k_uncat&#39;: exp(log_θtp1[3])} return θtp1 def compute_cov(chain): return np.log10(pd.DataFrame(chain)).cov() . chain, samples, accept_ratio, cov = MH_MCMC() . NameError Traceback (most recent call last) &lt;ipython-input-160-214059389194&gt; in &lt;module&gt; -&gt; 1 chain, samples, accept_ratio, cov = MH_MCMC() &lt;ipython-input-159-2ff5d01e632d&gt; in MH_MCMC(chain_length) 11 πt = -likelihood_logpdf(ut) - prior_logpdf(**θt) 12 &gt; 13 cov = prior_cov 14 i = 0 15 accept_ratio = 0 NameError: name &#39;prior_cov&#39; is not defined . ax = chain.plot.line(ylim=(0, 2300), xlim=(0,1000), logy=True) for i, param in enumerate(params): ax.axhline(param, lw=0.5, color=cmap(i), linestyle=&#39;--&#39;) . /usr/local/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:479: UserWarning: Attempted to set non-positive bottom ylim on a log-scaled axis. Invalid limit will be ignored. ax.set_ylim(self.ylim) . params . array([ 200., 2000., 100., 100.]) . sns.pairplot(chain, kind=&quot;kde&quot;) . &lt;seaborn.axisgrid.PairGrid at 0x1372078d0&gt; . accept_ratio . 0.04 . pd.DataFrame(cov) . 0 1 2 3 . 0 2098.532183 | -997.484455 | 123.922103 | -1823.662515 | . 1 -997.484455 | 873.417648 | -123.266991 | 331.330642 | . 2 123.922103 | -123.266991 | 23.117602 | 14.054959 | . 3 -1823.662515 | 331.330642 | 14.054959 | 2684.146271 | . ax = obs.plot.line(marker=&#39;o&#39;, lw=0.1, color=cmap(3), ylim=(-300, 5800), legend=True) samples.loc[-100:][samples.columns[::50]].T.plot.line(color=&#39;orange&#39;, lw=0.1, ax=ax, legend=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x139049d90&gt; . chain.to_hdf(&#39;chain.hdf&#39;, &#39;chain1&#39;) samples.to_hdf(&#39;chain.hdf&#39;, &#39;samples1&#39;) . 3.2 Frequentist Approach: Inference by Optimization . 4. Conclusions . Michaelis menten isn&#39;t even right, that&#39;s why the databases are such a mess. You need the right scheme, and the right equations . 5. References . A Database of Thermodynamic Quantities for the Reactions of Glycolysis and the Tricarboxylic Acid Cycle . | A database of thermodynamic properties of the reactions of glycolysis, the tricarboxylic acid cycle, and the pentose phosphate pathway . | . Thermodynamics of Glycolysis/III%3A_Reactivity_in_Organic_Biological_and_Inorganic_Chemistry_1/08%3A_Mechanisms_of_Glycolysis/8.08%3A_Thermodynamics_of_Glycolysis) | . BKMS-React database entry for glycolysis | . Bistability in Glycolysis Pathway as a Physiological Switch in Energy Metabolism | . Determination of the rate of hexokinase-glucose dissociation by the isotope-trapping method | . Quantitative Fundamentals of Molecular and Cellular Bioengineering | .",
            "url": "https://alexlenail.me/back_of_my_envelope/2021/01/28/Enzyme-Kinetic-Parameter-Inference.html",
            "relUrl": "/2021/01/28/Enzyme-Kinetic-Parameter-Inference.html",
            "date": " • Jan 28, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Enzyme Kinetic Parameter Inference",
            "content": "# imports from itertools import combinations_with_replacement, product from collections import Counter from io import StringIO import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.ticker as mtick import scipy.stats import seaborn as sns from scipy.stats import multivariate_normal from scipy.interpolate import interp1d from scipy.stats.kde import gaussian_kde from sklearn.linear_model import LinearRegression from sklearn.gaussian_process.kernels import Matern import ipywidgets as widgets from IPython.display import display %config InlineBackend.figure_format = &#39;retina&#39; %matplotlib inline plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] plt.rcParams[&#39;figure.dpi&#39;] = 140 plt.rcParams[&#39;agg.path.chunksize&#39;] = 10000 %load_ext jupyternotify import os # os.system(&quot;printf &#39; a&#39;&quot;) # or &#39; 7&#39; exp = np.exp sin = np.sin cos = np.cos sqrt = np.sqrt Π = np.prod π = np.pi N = np.random.normal def is_power_of_2(x): return x &gt; 4 and (x &amp; (x-1) == 0) . # simple matrix view macro import PIL.Image from matplotlib.pyplot import imshow def show(*Ms): fig, axs = plt.subplots(ncols=len(Ms)) for M, ax in zip(Ms, (axs if type(axs) == list else [axs])): M = np.atleast_2d(M) ax.imshow(PIL.Image.fromarray((M - M.min()) * 255 / M.max())) ax.set_xticks([0, M.shape[1]]) ax.set_yticks([0, M.shape[0]]) for side in [&quot;right&quot;,&quot;top&quot;,&quot;bottom&quot;,&quot;left&quot;]: ax.spines[side].set_visible(False) return fig, axs . # simple distrib pdf view macro from scipy.stats import weibull_min, dweibull, dgamma, expon as exponential from scipy.stats import rv_discrete, rv_continuous def is_discrete(dist): if hasattr(dist, &#39;dist&#39;): return isinstance(dist.dist, rv_discrete) else: return isinstance(dist, rv_discrete) def is_continuous(dist): if hasattr(dist, &#39;dist&#39;): return isinstance(dist.dist, rv_continuous) else: return isinstance(dist, rv_continuous) def plot_distrib(distrib, title=None): fig, ax = plt.subplots(1, 1) if is_continuous(distrib): x = np.linspace(distrib.ppf(0.01), distrib.ppf(0.99), 100) ax.plot(x, distrib.pdf(x), &#39;k-&#39;, lw=0.4) elif is_discrete(distrib): x = np.arange(distrib.ppf(0.01), distrib.ppf(0.99)) ax.plot(x, distrib.pmf(x), &#39;bo&#39;, ms=2, lw=0.4) r = distrib.rvs(size=1000) ax.hist(r, density=True, histtype=&#39;stepfilled&#39;, alpha=0.2, bins=100) if title: ax.set_title(title) return ax . Background . $$ newcommand{ kon}{k_{ mathrm{on}}} newcommand{ koff}{k_{ mathrm{off}}} newcommand{ kcat}{k_{ mathrm{cat}}} newcommand{ kuncat}{k_{ mathrm{uncat}}} newcommand{ dSdt}{ frac{d[ mathrm{S}]}{dt}} newcommand{ dEdt}{ frac{d[ mathrm{E}]}{dt}} newcommand{ dESdt}{ frac{d[ mathrm{ES}]}{dt}} newcommand{ dPdt}{ frac{d[ mathrm{P}]}{dt}}$$Chemical reactions in cells are catalyzed by enzymes, which confer rate accelerations to those reactions. Mathematical biologists seek to model enzyme-catalyzed chemical kinetics with differential equations. Consider the following formalism for enzymatically-catalyzed chemical reactions: . $$ E+S underset{ koff}{ overset{ kon}{ rightleftarrows}} ES underset{ kuncat}{ overset{ kcat}{ rightleftarrows}}E+P $$ . In this scheme E is an enzyme, S is its substrate, ES is the enzyme-substrate complex, which is an intermediate, and P is the product of the reaction. Each of those chemical species has a concentration in a fixed volume, which we denote with brackets (e.g. [E]). Four reactions are included: enzyme-substrate association, dissociation, enzyme catalysis of substrate into product, and enzyme-product re-association (uncatalysis). Each reaction has a linear rate in the concentrations with associated rate constants: $k_{ mathrm{on}}$, $k_{ mathrm{off}}$, $k_{ mathrm{cat}}$, $k_{ mathrm{uncat}}$ which comprise the 4 degrees of freedom in our kinetics. The &#39;direction&#39; of the reaction, and the designation of &#39;substrate&#39; and &#39;product&#39; is our choice -- the model is entirely symmetric, which is reflected in the associated ODEs: . $$ begin{aligned} frac{d[ mathrm{S}]}{dt} &amp;= k_{ mathrm{off}}[ mathrm{ES}] - k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] frac{d[ mathrm{E}]}{dt} &amp;= k_{ mathrm{off}}[ mathrm{ES}] - k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] + k_{ mathrm{cat}}[ mathrm{ES}] - k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] frac{d[ mathrm{ES}]}{dt} &amp;= - k_{ mathrm{off}}[ mathrm{ES}] + k_{ mathrm{on}}[ mathrm{E}][ mathrm{S}] - k_{ mathrm{cat}}[ mathrm{ES}] + k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] frac{d[ mathrm{P}]}{dt} &amp;= k_{ mathrm{cat}}[ mathrm{ES}] - k_{ mathrm{uncat}}[ mathrm{E}][ mathrm{P}] end{aligned}$$The assumption made to derive these equations is that the molecular species are &#39;well-mixed&#39; in solution, permitting us to invoke the &#39;Law of Mass Action&#39;. We note here that the intracellular environment is not best described as well-mixed, and models of &#39;Macromolecular Crowding&#39; have led to more accurate rate laws for these reactions in vivo. However, we will retain the well-mixed assumption in this work. . We are typically most interested in integrating $ frac{d[ mathrm{P}]}{dt}$. . An assumption commonly made at this point is to suppose that the rates of enzyme-substrate association ($ kon$) and dissociation ($ koff$) are greater than those for the enzyme and product ($ kon$, $ koff$ $ gg$ $ kcat$, $ kuncat$). Under that assumption, and assuming the initial substrate concentration is much larger than the enzyme concentration ($[ mathrm{S_0}] &gt; [ mathrm{E_0}]$), a timescale separation argument called the &quot;Quasi-Steady-State Approximation&quot; (QSSA) permits us to set $ dESdt = 0$. From this approximation, we can derive footnote{A good Reversible Michaelis-Menten/Briggs-Haldane derivation can be found here the traditional Reversible Michaelis-Menten/Briggs-Haldane expression: . $$ frac{d[ mathrm{P}]}{dt} = frac{ frac{ kcat , [ mathrm{E_T}] [ mathrm{S}]}{K_{m, mathrm{S}}} - frac{ kuncat , [ mathrm{E_T}] [ mathrm{P}]}{K_{m, mathrm{P}}}} {1+ frac{[ mathrm{S}]}{K_{m, mathrm{S}}} + frac{[ mathrm{P}]}{K_{m, mathrm{P}}}}$$in which we have introduced the &quot;Michaelis Constants&quot;: $K_{m, mathrm{S}} = frac{ koff + kcat}{ kon}$ and $K_{m, mathrm{P}} = frac{ kcat + koff}{ kuncat}$. The determination of the 4 constants ($ kcat$, $ kuncat$, $K_{m, mathrm{S}}$, and $K_{m, mathrm{P}}$) from experimental data for the enzymatic reactions of life is a major project, and reported values have been tabulated in the BRENDA database. . 1. Simulation . # define integrate_full k_on = 1 k_off = 100 k_cat = 100 k_uncat = 0.1 def integrate_full(E, S, dt, steps, k_on=k_on, k_off=k_off, k_cat=k_cat, k_uncat=k_uncat): S = S E_T = E ES = 0 P = 0 P2 = 0 traj = [[S, E, ES, P, P2]] for _ in range(int(steps)): dSdt = k_off * ES - k_on * E * S dEdt = k_off * ES - k_on * E * S + k_cat * ES - k_uncat * E * P dESdt = k_on * E * S - k_off * ES - k_cat * ES + k_uncat * E * P dPdt = k_cat * ES - k_uncat * E * P dP2dt = ((k_cat * E_T * S) / kms - (k_off * E_T * P) / kmp) / (1 + S / kms + P / kmp) S += dSdt * dt E += dEdt * dt ES += dESdt * dt P += dPdt * dt P2 += dP2dt * dt traj.append([S, E, ES, P, P2]) return traj . E = 1e3 S = 5.5e3 dt = 1e-6 steps = 1e5 traj = pd.DataFrame(integrate_full(E, S, dt, steps), columns=[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;, &#39;P_MM&#39;]) . params = f&#39;[k_on= {k_on}/μM x s] [k_off = {k_off}/s] [k_cat = {k_cat}/s] [k_uncat = {k_uncat}/μM x s] [E₀ = {int(E)}μM] [S₀ = {int(S)}μM]&#39; traj.plot.line(title=params) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1394c4890&gt; . cmap = plt.get_cmap(&quot;tab10&quot;) params = f&#39;[k_on= ? /μM x s] [k_off = ? /s] [k_cat = ? /s] [k_uncat = ? /μM x s] [E₀ = ? μM] [S₀ = ? μM]&#39; obs = traj[&#39;P&#39;][traj[&#39;P&#39;].index % 10000 == 0] obs.plot.line(title=params, marker=&#39;o&#39;, lw=0.1, color=cmap(3), ylim=(-300, 5800), legend=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x13052fd50&gt; . def integrate_MM(zt, dt=dt, steps=steps, E_T=1e3, S_0=5.5e3): kms, kmp, k_cat, k_off = zt S = S_0 E_T = E_T P = 0 traj = [[S, P]] for _ in range(int(steps)): dPdt = ((k_cat * E_T * S) / kms - (k_off * E_T * P) / kmp) / (1 + S / kms + P / kmp) dSdt = -dPdt S += dSdt * dt P += dPdt * dt traj.append([S, P]) return pd.DataFrame(traj, columns=[&#39;S&#39;, &#39;P&#39;]) . k_on = 1 # in micromolar k_off = 100 # in 1/s k_cat = 100 # in 1/s k_uncat = 0.1 # in micromolar kms = (k_off + k_cat) / k_on kmp = (k_off + k_cat) / k_uncat # koff = kmp * kuncat - kcat # kon = (koff + kcat) / kms kms, kmp . (200.0, 2000.0) . 2. Inference . 2.1 Basic . # if kon is 1 micromolar, and affinities can range from nanomolar to milimolar, then k_off must be between 1e-3 and 1e3. lognormal # kon between 5e5 and 5e6 mean at 1e6, lognormal # kuncat zero to 1e6, lognormal, mean at 1e2 # KM lies between 10^-1 and 10^-7 M scipy.stats.lognorm? . params = np.array([kms, kmp, k_cat, k_off]) prior_cov = np.eye(4) * params/3 prior = multivariate_normal(mean=params, cov=prior_cov) observational_noise = 1 likelihood_dist = multivariate_normal(mean=obs.values, cov=np.eye(len(obs)) * observational_noise) observed_idx = obs.index.astype(int) def MH_MCMC(chain_length=1e3): zt = prior.rvs() ut = integrate_MM(zt)[&#39;P&#39;] πt = -likelihood_dist.logpdf(ut.loc[observed_idx].values) #- prior.logpdf(zt) cov = prior_cov i = 0 accept_ratio = 0 chain = [] samples = [] while i &lt; chain_length: ztp1 = np.clip(np.random.multivariate_normal(zt, cov), 0, None) utp1 = integrate_MM(ztp1)[&#39;P&#39;] πtp1 = -likelihood_dist.logpdf(utp1.loc[observed_idx].values) #- prior.logpdf(zt) if πtp1 - πt &lt; -np.log(np.random.rand()): zt, ut, πt = ztp1, utp1, πtp1 accept_ratio += 1 chain.append(zt) samples.append(ut.values) i += 1 if i % 1e2 == 0 and i &gt; 0: cov = np.cov(np.array(chain), rowvar=False) print(i, end=&#39; r&#39;) return pd.DataFrame(chain, columns=[&#39;kms&#39;, &#39;kmp&#39;, &#39;k_cat&#39;, &#39;k_off&#39;]), pd.DataFrame(samples), accept_ratio/chain_length, cov . chain, samples, accept_ratio, cov = MH_MCMC() . 1000 . ax = chain.plot.line(ylim=(0, 2300), xlim=(0,1000), logy=True) for i, param in enumerate(params): ax.axhline(param, lw=0.5, color=cmap(i), linestyle=&#39;--&#39;) . /usr/local/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py:479: UserWarning: Attempted to set non-positive bottom ylim on a log-scaled axis. Invalid limit will be ignored. ax.set_ylim(self.ylim) . params . array([ 200., 2000., 100., 100.]) . sns.pairplot(chain, kind=&quot;kde&quot;) . &lt;seaborn.axisgrid.PairGrid at 0x1372078d0&gt; . accept_ratio . 0.04 . pd.DataFrame(cov) . 0 1 2 3 . 0 2098.532183 | -997.484455 | 123.922103 | -1823.662515 | . 1 -997.484455 | 873.417648 | -123.266991 | 331.330642 | . 2 123.922103 | -123.266991 | 23.117602 | 14.054959 | . 3 -1823.662515 | 331.330642 | 14.054959 | 2684.146271 | . ax = obs.plot.line(marker=&#39;o&#39;, lw=0.1, color=cmap(3), ylim=(-300, 5800), legend=True) samples.loc[-100:][samples.columns[::50]].T.plot.line(color=&#39;orange&#39;, lw=0.1, ax=ax, legend=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x139049d90&gt; . chain.to_hdf(&#39;chain.hdf&#39;, &#39;chain1&#39;) samples.to_hdf(&#39;chain.hdf&#39;, &#39;samples1&#39;) . 2.2 Enzyme/Substrate Concentration Ratio Assumption . E = 5.5e3 S = 5.5e3 dt = 1e-6 steps = 1e5 traj = pd.DataFrame(integrate_full(E, S, dt, steps), columns=[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;, &#39;P_MM&#39;]) . params = f&#39;[k_on= {k_on}/μM x s] [k_off = {k_off}/s] [k_cat = {k_cat}/s] [k_uncat = {k_uncat}/μM x s] [E₀ = {int(E)}μM] [S₀ = {int(S)}μM]&#39; traj.plot.line(title=params) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x139b14790&gt; . 2.3 Quasi-Steady-State Approximation (QSSA) . ES takes a while to equilibrate . E = 5.5e3 S = 5.5e3 dt = 1e-6 steps = 1e5 k_on = 0.1 # in micromolar k_off = 10 # in 1/s k_cat = 100 # in 1/s k_uncat = 0.1 # in micromolar kms = (k_off + k_cat) / k_on kmp = (k_off + k_cat) / k_uncat kms, kmp traj = pd.DataFrame(integrate_full(E, S, dt, steps, k_on=k_on, k_off=k_off, k_cat=k_cat, k_uncat=k_uncat), columns=[&#39;S&#39;, &#39;E&#39;, &#39;ES&#39;, &#39;P&#39;, &#39;P_MM&#39;]) . params = f&#39;[k_on= {k_on}/μM x s] [k_off = {k_off}/s] [k_cat = {k_cat}/s] [k_uncat = {k_uncat}/μM x s] [E₀ = {int(E)}μM] [S₀ = {int(S)}μM]&#39; traj.plot.line(title=params) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x13d5f63d0&gt; . 3. References . A Database of Thermodynamic Quantities for the Reactions of Glycolysis and the Tricarboxylic Acid Cycle . | A database of thermodynamic properties of the reactions of glycolysis, the tricarboxylic acid cycle, and the pentose phosphate pathway . | . Thermodynamics of Glycolysis/III%3A_Reactivity_in_Organic_Biological_and_Inorganic_Chemistry_1/08%3A_Mechanisms_of_Glycolysis/8.08%3A_Thermodynamics_of_Glycolysis) | . BKMS-React database entry for glycolysis | . Bistability in Glycolysis Pathway as a Physiological Switch in Energy Metabolism | . Determination of the rate of hexokinase-glucose dissociation by the isotope-trapping method | . Quantitative Fundamentals of Molecular and Cellular Bioengineering | .",
            "url": "https://alexlenail.me/back_of_my_envelope/2021/01/01/Enzyme-Kinetic-Parameter-Inference.html",
            "relUrl": "/2021/01/01/Enzyme-Kinetic-Parameter-Inference.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Orthogonal Functions",
            "content": "# imports import numpy as np import pandas as pd import matplotlib.pyplot as plt import scipy.stats %matplotlib inline plt.rcParams[&#39;figure.figsize&#39;] = [12, 5] plt.rcParams[&#39;figure.dpi&#39;] = 140 π = np.pi exp = np.exp sin = np.sin cos = np.cos sqrt = np.sqrt . Fourier Basis . grid = 200 domain = [0, 2*π] dx = (domain[1]-domain[0])/grid grid = np.linspace(*domain, grid) def fourier(k, x): return sin(k*x)+cos(k*x) . n = 5 basis = pd.DataFrame({k: fourier(k, grid) for k in range(1,n)}, index=grid) ax = basis.plot.line(lw=0.4, xlim=domain) ax.axhline(0, c=&#39;black&#39;, lw=&#39;0.3&#39;) . &lt;matplotlib.lines.Line2D at 0x136a4e890&gt; . from scipy import integrate def compare_two(i, j): product = pd.Series(basis[i]*basis[j], name=&#39;product&#39;) product = pd.DataFrame([basis[i], basis[j], product]).T ax = product.plot.line(lw=0.5, color=[&#39;red&#39;, &#39;blue&#39;, &#39;purple&#39;]) ax.fill_between(grid, product[&#39;product&#39;], alpha=0.1) return integrate.trapz(product[&#39;product&#39;], x=product.index) . print(&#39;integral =&#39;, np.round(compare_two(3,4), 4)) . integral = -0.0 . &quot;fourier modes as eigenfunctions of the derivative operator&quot; What? . Polynomial Bases .",
            "url": "https://alexlenail.me/back_of_my_envelope/2020/12/04/orthogonal-functions.html",
            "relUrl": "/2020/12/04/orthogonal-functions.html",
            "date": " • Dec 4, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "",
          "url": "https://alexlenail.me/back_of_my_envelope/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Old Blog",
          "content": "",
          "url": "https://alexlenail.me/back_of_my_envelope/old/",
          "relUrl": "/old/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page7": {
          "title": "",
          "content": "{“/about/”:”http://alexlenail.me”,”/old/”:”https://alexlenail.medium.com/”} .",
          "url": "https://alexlenail.me/back_of_my_envelope/redirects.json",
          "relUrl": "/redirects.json",
          "date": ""
      }
      
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://alexlenail.me/back_of_my_envelope/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}